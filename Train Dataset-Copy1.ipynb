{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7c8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbdcd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mv ../drone_dataset.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dcf0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade protobuf==3.20.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb98bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install transformers==4.5.1\n",
    "#!pip3 install -U tokenizers\n",
    "# The code below just solve many problems lol\n",
    "#!pip3 uninstall tokenizers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc16738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/CARLA_0.9.8/PythonAPI/carla/dist/carla-0.9.8-py3.5-linux-x86_64.egg/carla/libcarla.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "import d4rl\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from replay_buffer import ReplayBuffer\n",
    "from lamb import Lamb\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from pathlib import Path\n",
    "from data import create_dataloader\n",
    "from decision_transformer.models.decision_transformer import DecisionTransformer\n",
    "from evaluation import create_vec_eval_episodes_fn, vec_evaluate_episode_rtg\n",
    "from trainer import SequenceTrainer\n",
    "from logger import Logger\n",
    "\n",
    "from env import make_pytorch_env\n",
    "\n",
    "#MAX_EPISODE_LEN = 2000 # 4000 # 2000 # 4000 # Warning: there is a similar variable in data.py! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8021a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", type=int, default=10)\n",
    "parser.add_argument(\"--env\", type=str, default=\"drone_dataset\")\n",
    "\n",
    "# model options\n",
    "context_length = 400 #330\n",
    "## Training Context Length K: (default: 20)\n",
    "parser.add_argument(\"--K\", type=int, default=context_length) \n",
    "## Embedding dimension: (default: 512)\n",
    "parser.add_argument(\"--embed_dim\", type=int, default=512)\n",
    "## Number of Layers: (default: 4)\n",
    "parser.add_argument(\"--n_layer\", type=int, default=4)\n",
    "## Number of Attention Heads: (default: 4)\n",
    "parser.add_argument(\"--n_head\", type=int, default=4)\n",
    "## Nonlinearity function: \n",
    "parser.add_argument(\"--activation_function\", type=str, default=\"relu\")\n",
    "## Dropout:\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "## Evaluating Context Length K: \n",
    "parser.add_argument(\"--eval_context_length\", type=int, default=context_length)\n",
    "## Positional embedding: absolute ordering\n",
    "parser.add_argument(\"--ordering\", type=int, default=1) # 0\n",
    "\n",
    "# shared evaluation options\n",
    "# g_eval: (default: 3600)\n",
    "parser.add_argument(\"--eval_rtg\", type=int, default=6000)\n",
    "parser.add_argument(\"--num_eval_episodes\", type=int, default=10)\n",
    "\n",
    "# shared training options\n",
    "parser.add_argument(\"--init_temperature\", type=float, default=0.1)\n",
    "## Batch Size: (default: 256)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "parser.add_argument(\"--learning_rate\", \"-lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--weight_decay\", \"-wd\", type=float, default=5e-4)\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=10000)\n",
    "\n",
    "# pretraining options\n",
    "parser.add_argument(\"--max_pretrain_iters\", type=int, default=1)\n",
    "parser.add_argument(\"--num_updates_per_pretrain_iter\", type=int, default=5000)\n",
    "\n",
    "# finetuning options\n",
    "parser.add_argument(\"--max_online_iters\", type=int, default=1500)\n",
    "parser.add_argument(\"--online_rtg\", type=int, default=7200)\n",
    "parser.add_argument(\"--num_online_rollouts\", type=int, default=1)\n",
    "parser.add_argument(\"--replay_size\", type=int, default=1000)\n",
    "parser.add_argument(\"--num_updates_per_online_iter\", type=int, default=300)\n",
    "parser.add_argument(\"--eval_interval\", type=int, default=10)\n",
    "\n",
    "# environment options\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "parser.add_argument(\"--log_to_tb\", \"-w\", type=bool, default=True)\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"./exp\")\n",
    "parser.add_argument(\"--exp_name\", type=str, default=\"default\")\n",
    "\n",
    "# general options\n",
    "parser.add_argument(\"--max_episode_len\", type=int, default=2000)\n",
    "\n",
    "# Add for fast Debbuging\n",
    "'''\n",
    "parser.add_argument(\"--K\", type=int, default=1)#####40)\n",
    "parser.add_argument(\"--embed_dim\", type=int, default=4)#####512)\n",
    "parser.add_argument(\"--n_layer\", type=int, default=2)#####8)\n",
    "parser.add_argument(\"--n_head\", type=int, default=2)#####8)\n",
    "parser.add_argument(\"--eval_context_length\", type=int, default=1)#####10) # d:5\n",
    "parser.add_argument(\"--eval_rtg\", type=int, default=600)#####6000)\n",
    "parser.add_argument(\"--num_eval_episodes\", type=int, default=2)#####10)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4)#####256)\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=10)#####10000)\n",
    "parser.add_argument(\"--num_updates_per_pretrain_iter\", type=int, default=500)#####5000)\n",
    "parser.add_argument(\"--max_online_iters\", type=int, default=500)#####1500)\n",
    "parser.add_argument(\"--online_rtg\", type=int, default=720)#####7200)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b539c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, variant):\n",
    "\n",
    "        self.state_dim, self.act_dim, self.action_range = self._get_env_spec(variant)\n",
    "        self.offline_trajs, self.state_mean, self.state_std = self._load_dataset(\n",
    "            variant[\"env\"]\n",
    "        )\n",
    "        # initialize by offline trajs\n",
    "        self.replay_buffer = ReplayBuffer(variant[\"replay_size\"], self.offline_trajs)\n",
    "\n",
    "        self.aug_trajs = []\n",
    "\n",
    "        self.device = variant.get(\"device\", \"cuda\")\n",
    "        self.target_entropy = -self.act_dim\n",
    "        self.model = DecisionTransformer(\n",
    "            state_dim=self.state_dim,\n",
    "            act_dim=self.act_dim,\n",
    "            action_range=self.action_range,\n",
    "            max_length=variant[\"K\"],\n",
    "            eval_context_length=variant[\"eval_context_length\"],\n",
    "            max_ep_len=variant[\"max_episode_len\"],\n",
    "            hidden_size=variant[\"embed_dim\"],\n",
    "            n_layer=variant[\"n_layer\"],\n",
    "            n_head=variant[\"n_head\"],\n",
    "            n_inner=4 * variant[\"embed_dim\"],\n",
    "            activation_function=variant[\"activation_function\"],\n",
    "            n_positions=1024,\n",
    "            resid_pdrop=variant[\"dropout\"],\n",
    "            attn_pdrop=variant[\"dropout\"],\n",
    "            stochastic_policy=True,\n",
    "            ordering=variant[\"ordering\"],\n",
    "            init_temperature=variant[\"init_temperature\"],\n",
    "            target_entropy=self.target_entropy,\n",
    "        ).to(device=self.device)\n",
    "\n",
    "        self.optimizer = Lamb(\n",
    "            self.model.parameters(),\n",
    "            lr=variant[\"learning_rate\"],\n",
    "            weight_decay=variant[\"weight_decay\"],\n",
    "            eps=1e-8,\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.optimizer, lambda steps: min((steps + 1) / variant[\"warmup_steps\"], 1)\n",
    "        )\n",
    "\n",
    "        self.log_temperature_optimizer = torch.optim.Adam(\n",
    "            [self.model.log_temperature],\n",
    "            lr=1e-4,\n",
    "            betas=[0.9, 0.999],\n",
    "        )\n",
    "\n",
    "        # track the training progress and\n",
    "        # training/evaluation/online performance in all the iterations\n",
    "        self.pretrain_iter = 0\n",
    "        self.online_iter = 0\n",
    "        self.total_transitions_sampled = 0\n",
    "        self.variant = variant\n",
    "        self.reward_scale = 1.0 if \"antmaze\" in variant[\"env\"] else 0.001\n",
    "        self.logger = Logger(variant)\n",
    "\n",
    "    def _get_env_spec(self, variant):\n",
    "        #####env = gym.make(variant[\"env\"])\n",
    "        env = make_pytorch_env(args)\n",
    "        #env.max_step = MAX_EPISODE_LEN\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.shape[0]\n",
    "        #action_range = [-0.999999, 0.999999]\n",
    "        \n",
    "        action_range = [\n",
    "            float(env.action_space.low.min()) + 1e-6,\n",
    "            float(env.action_space.high.max()) - 1e-6,\n",
    "        ]\n",
    "        \n",
    "        print(\"action_range: {}\".format(action_range))\n",
    "        env.close()\n",
    "        return state_dim, act_dim, action_range\n",
    "\n",
    "    def _save_model(self, path_prefix, is_pretrain_model=False):\n",
    "        to_save = {\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": self.scheduler.state_dict(),\n",
    "            \"pretrain_iter\": self.pretrain_iter,\n",
    "            \"online_iter\": self.online_iter,\n",
    "            \"args\": self.variant,\n",
    "            \"total_transitions_sampled\": self.total_transitions_sampled,\n",
    "            \"np\": np.random.get_state(),\n",
    "            \"python\": random.getstate(),\n",
    "            \"pytorch\": torch.get_rng_state(),\n",
    "            \"log_temperature_optimizer_state_dict\": self.log_temperature_optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        with open(f\"{path_prefix}/model.pt\", \"wb\") as f:\n",
    "            torch.save(to_save, f)\n",
    "        print(f\"\\nModel saved at {path_prefix}/model.pt\")\n",
    "\n",
    "        if is_pretrain_model:\n",
    "            with open(f\"{path_prefix}/pretrain_model.pt\", \"wb\") as f:\n",
    "                torch.save(to_save, f)\n",
    "            print(f\"Model saved at {path_prefix}/pretrain_model.pt\")\n",
    "\n",
    "    def _load_model(self, path_prefix):\n",
    "        if Path(f\"{path_prefix}/model.pt\").exists():\n",
    "            with open(f\"{path_prefix}/model.pt\", \"rb\") as f:\n",
    "                checkpoint = torch.load(f)\n",
    "            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "            self.log_temperature_optimizer.load_state_dict(\n",
    "                checkpoint[\"log_temperature_optimizer_state_dict\"]\n",
    "            )\n",
    "            self.pretrain_iter = checkpoint[\"pretrain_iter\"]\n",
    "            self.online_iter = checkpoint[\"online_iter\"]\n",
    "            self.total_transitions_sampled = checkpoint[\"total_transitions_sampled\"]\n",
    "            np.random.set_state(checkpoint[\"np\"])\n",
    "            random.setstate(checkpoint[\"python\"])\n",
    "            torch.set_rng_state(checkpoint[\"pytorch\"])\n",
    "            print(f\"Model loaded at {path_prefix}/model.pt\")\n",
    "\n",
    "    def _load_dataset(self, env_name):\n",
    "\n",
    "        dataset_path = f\"./data/{env_name}.pkl\"\n",
    "        with open(dataset_path, \"rb\") as f:\n",
    "            trajectories = pickle.load(f)\n",
    "\n",
    "        states, traj_lens, returns = [], [], []\n",
    "        for path in trajectories:\n",
    "            states.append(path[\"observations\"])\n",
    "            traj_lens.append(len(path[\"observations\"]))\n",
    "            returns.append(path[\"rewards\"].sum())\n",
    "        traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "        # used for input normalization\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        num_timesteps = sum(traj_lens)\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Starting new experiment: {env_name}\")\n",
    "        print(f\"{len(traj_lens)} trajectories, {num_timesteps} timesteps found\")\n",
    "        print(f\"Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}\")\n",
    "        print(f\"Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}\")\n",
    "        print(f\"Average length: {np.mean(traj_lens):.2f}, std: {np.std(traj_lens):.2f}\")\n",
    "        print(f\"Max length: {np.max(traj_lens):.2f}, min: {np.min(traj_lens):.2f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "        num_trajectories = 1\n",
    "        timesteps = traj_lens[sorted_inds[-1]]\n",
    "        ind = len(trajectories) - 2\n",
    "        while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] < num_timesteps:\n",
    "            timesteps += traj_lens[sorted_inds[ind]]\n",
    "            num_trajectories += 1\n",
    "            ind -= 1\n",
    "        sorted_inds = sorted_inds[-num_trajectories:]\n",
    "        trajectories = [trajectories[ii] for ii in sorted_inds]\n",
    "\n",
    "        return trajectories, state_mean, state_std\n",
    "\n",
    "    def _augment_trajectories(\n",
    "        self,\n",
    "        online_envs,\n",
    "        target_explore,\n",
    "        n,\n",
    "        randomized=False,\n",
    "    ):\n",
    "\n",
    "        max_ep_len = self.variant[\"max_episode_len\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generate init state\n",
    "            target_return = [target_explore * self.reward_scale] * online_envs.num_envs\n",
    "\n",
    "            returns, lengths, trajs = vec_evaluate_episode_rtg(\n",
    "                online_envs,\n",
    "                self.state_dim,\n",
    "                self.act_dim,\n",
    "                self.model,\n",
    "                max_ep_len=max_ep_len,\n",
    "                reward_scale=self.reward_scale,\n",
    "                target_return=target_return,\n",
    "                mode=\"normal\",\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                device=self.device,\n",
    "                use_mean=False,\n",
    "            )\n",
    "\n",
    "        self.replay_buffer.add_new_trajs(trajs)\n",
    "        self.aug_trajs += trajs\n",
    "        self.total_transitions_sampled += np.sum(lengths)\n",
    "\n",
    "        return {\n",
    "            \"aug_traj/return\": np.mean(returns),\n",
    "            \"aug_traj/length\": np.mean(lengths),\n",
    "        }\n",
    "\n",
    "    def pretrain(self, eval_envs, loss_fn):\n",
    "        print(\"\\n\\n\\n*** Pretrain ***\")\n",
    "        print(\"----------------\")\n",
    "        print(\"eval_envs: {}\".format(eval_envs))\n",
    "        print(\"loss_fn: {}\".format(loss_fn))\n",
    "        \n",
    "        eval_fns = [\n",
    "            create_vec_eval_episodes_fn(\n",
    "                vec_env=eval_envs,\n",
    "                eval_rtg=self.variant[\"eval_rtg\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                device=self.device,\n",
    "                use_mean=True,\n",
    "                reward_scale=self.reward_scale,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        trainer = SequenceTrainer(\n",
    "            model=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            log_temperature_optimizer=self.log_temperature_optimizer,\n",
    "            scheduler=self.scheduler,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        writer = (\n",
    "            SummaryWriter(self.logger.log_path) if self.variant[\"log_to_tb\"] else None\n",
    "        )\n",
    "        while self.pretrain_iter < self.variant[\"max_pretrain_iters\"]:\n",
    "            # in every iteration, prepare the data loader\n",
    "            dataloader = create_dataloader(\n",
    "                trajectories=self.offline_trajs,\n",
    "                num_iters=self.variant[\"num_updates_per_pretrain_iter\"],\n",
    "                batch_size=self.variant[\"batch_size\"],\n",
    "                max_len=self.variant[\"K\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                reward_scale=self.reward_scale,\n",
    "                action_range=self.action_range,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "\n",
    "            train_outputs = trainer.train_iteration(\n",
    "                loss_fn=loss_fn,\n",
    "                dataloader=dataloader,\n",
    "            )\n",
    "            eval_outputs, eval_reward = self.evaluate(eval_fns)\n",
    "            outputs = {\"time/total\": time.time() - self.start_time}\n",
    "            outputs.update(train_outputs)\n",
    "            outputs.update(eval_outputs)\n",
    "            self.logger.log_metrics(\n",
    "                outputs,\n",
    "                iter_num=self.pretrain_iter,\n",
    "                total_transitions_sampled=self.total_transitions_sampled,\n",
    "                writer=writer,\n",
    "            )\n",
    "\n",
    "            self._save_model(\n",
    "                path_prefix=self.logger.log_path,\n",
    "                is_pretrain_model=True,\n",
    "            )\n",
    "\n",
    "            self.pretrain_iter += 1\n",
    "\n",
    "    def evaluate(self, eval_fns):\n",
    "        eval_start = time.time()\n",
    "        self.model.eval()\n",
    "        outputs = {}\n",
    "        for eval_fn in eval_fns:\n",
    "            o = eval_fn(self.model)\n",
    "            outputs.update(o)\n",
    "        outputs[\"time/evaluation\"] = time.time() - eval_start\n",
    "\n",
    "        eval_reward = outputs[\"evaluation/return_mean_gm\"]\n",
    "        return outputs, eval_reward\n",
    "\n",
    "    def online_tuning(self, online_envs, eval_envs, loss_fn):\n",
    "\n",
    "        print(\"\\n\\n\\n*** Online Finetuning ***\")\n",
    "\n",
    "        trainer = SequenceTrainer(\n",
    "            model=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            log_temperature_optimizer=self.log_temperature_optimizer,\n",
    "            scheduler=self.scheduler,\n",
    "            device=self.device,\n",
    "        )\n",
    "        eval_fns = [\n",
    "            create_vec_eval_episodes_fn(\n",
    "                vec_env=eval_envs,\n",
    "                eval_rtg=self.variant[\"eval_rtg\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                device=self.device,\n",
    "                use_mean=True,\n",
    "                reward_scale=self.reward_scale,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "        ]\n",
    "        writer = (\n",
    "            SummaryWriter(self.logger.log_path) if self.variant[\"log_to_tb\"] else None\n",
    "        )\n",
    "        while self.online_iter < self.variant[\"max_online_iters\"]:\n",
    "\n",
    "            outputs = {}\n",
    "            augment_outputs = self._augment_trajectories(\n",
    "                online_envs,\n",
    "                self.variant[\"online_rtg\"],\n",
    "                n=self.variant[\"num_online_rollouts\"],\n",
    "            )\n",
    "            outputs.update(augment_outputs)\n",
    "\n",
    "            dataloader = create_dataloader(\n",
    "                trajectories=self.replay_buffer.trajectories,\n",
    "                num_iters=self.variant[\"num_updates_per_online_iter\"],\n",
    "                batch_size=self.variant[\"batch_size\"],\n",
    "                max_len=self.variant[\"K\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                reward_scale=self.reward_scale,\n",
    "                action_range=self.action_range,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "\n",
    "            # finetuning\n",
    "            is_last_iter = self.online_iter == self.variant[\"max_online_iters\"] - 1\n",
    "            if (self.online_iter + 1) % self.variant[\n",
    "                \"eval_interval\"\n",
    "            ] == 0 or is_last_iter:\n",
    "                evaluation = True\n",
    "            else:\n",
    "                evaluation = False\n",
    "\n",
    "            train_outputs = trainer.train_iteration(\n",
    "                loss_fn=loss_fn,\n",
    "                dataloader=dataloader,\n",
    "            )\n",
    "            outputs.update(train_outputs)\n",
    "\n",
    "            if evaluation:\n",
    "                eval_outputs, eval_reward = self.evaluate(eval_fns)\n",
    "                outputs.update(eval_outputs)\n",
    "\n",
    "            outputs[\"time/total\"] = time.time() - self.start_time\n",
    "\n",
    "            # log the metrics\n",
    "            self.logger.log_metrics(\n",
    "                outputs,\n",
    "                iter_num=self.pretrain_iter + self.online_iter,\n",
    "                total_transitions_sampled=self.total_transitions_sampled,\n",
    "                writer=writer,\n",
    "            )\n",
    "\n",
    "            self._save_model(\n",
    "                path_prefix=self.logger.log_path,\n",
    "                is_pretrain_model=False,\n",
    "            )\n",
    "\n",
    "            self.online_iter += 1\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        utils.set_seed_everywhere(args.seed)\n",
    "\n",
    "        import d4rl\n",
    "\n",
    "        def loss_fn(\n",
    "            a_hat_dist,     # action_preds\n",
    "            a,              # action_target\n",
    "            attention_mask, # padding_mask\n",
    "            entropy_reg,    # self.model.temperature().detach()\n",
    "        ):\n",
    "            # a_hat is a SquashedNormal Distribution\n",
    "            log_likelihood = a_hat_dist.log_likelihood(a)[attention_mask > 0].mean()\n",
    "            \n",
    "            entropy = a_hat_dist.entropy().mean()\n",
    "            loss = -(log_likelihood + entropy_reg * entropy)\n",
    "            \n",
    "            '''\n",
    "            print(\"a_hat_dist : {}\".format(a_hat_dist))\n",
    "            print(\"a : {}\".format(a))\n",
    "            torch.save(a,\"a.pt\")\n",
    "            print(\"a_hat_dist.log_likelihood(a) : {}\".format(a_hat_dist.log_likelihood(a)))\n",
    "            #print(\"attention_mask : {}\".format(attention_mask))\n",
    "            print(\"log_likelihood: {}\".format(log_likelihood))\n",
    "            print(\"loss inside jupyter: {} of type: {}\".format(loss,type(loss)))\n",
    "            '''\n",
    "            \n",
    "            return (\n",
    "                loss,\n",
    "                -log_likelihood,\n",
    "                entropy,\n",
    "            )\n",
    "\n",
    "        def get_env_builder(seed, env_name, target_goal=None):\n",
    "            def make_env_fn():\n",
    "                import d4rl\n",
    "\n",
    "                #####env = gym.make(env_name)\n",
    "                env = make_pytorch_env(args)\n",
    "                #env.max_step = MAX_EPISODE_LEN\n",
    "                env.seed(seed)\n",
    "                '''\n",
    "                if hasattr(env.env, \"wrapped_env\"):\n",
    "                    env.env.wrapped_env.seed(seed)\n",
    "                elif hasattr(env.env, \"seed\"):\n",
    "                    env.env.seed(seed)\n",
    "                else:\n",
    "                    pass\n",
    "                '''\n",
    "                '''\n",
    "                env.action_space.seed(seed)\n",
    "                env.observation_space.seed(seed)\n",
    "                '''\n",
    "\n",
    "                if target_goal:\n",
    "                    env.set_target_goal(target_goal)\n",
    "                    print(f\"Set the target goal to be {env.target_goal}\")\n",
    "                return env\n",
    "\n",
    "            return make_env_fn\n",
    "\n",
    "        print(\"\\n\\nMaking Eval Env.....\")\n",
    "        env_name = self.variant[\"env\"]\n",
    "        if \"antmaze\" in env_name:\n",
    "            env = gym.make(env_name)\n",
    "            target_goal = env.target_goal\n",
    "            env.close()\n",
    "            print(f\"Generated the fixed target goal: {target_goal}\")\n",
    "        else:\n",
    "            target_goal = None\n",
    "        eval_envs = SubprocVecEnv(\n",
    "            [\n",
    "                get_env_builder(i, env_name=env_name, target_goal=target_goal)\n",
    "                for i in range(self.variant[\"num_eval_episodes\"])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        if self.variant[\"max_pretrain_iters\"]:\n",
    "            self.pretrain(eval_envs, loss_fn)\n",
    "        \n",
    "        if self.variant[\"max_online_iters\"]:\n",
    "            print(\"\\n\\nMaking Online Env.....\")\n",
    "            online_envs = SubprocVecEnv(\n",
    "                [\n",
    "                    get_env_builder(i + 100, env_name=env_name, target_goal=target_goal)\n",
    "                    for i in range(self.variant[\"num_online_rollouts\"])\n",
    "                ]\n",
    "            )\n",
    "            self.online_tuning(online_envs, eval_envs, loss_fn)\n",
    "            online_envs.close()\n",
    "\n",
    "        eval_envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a428b67f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/odt/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_range: [-0.999999, 0.999999]\n",
      "==================================================\n",
      "Starting new experiment: drone_dataset\n",
      "1254 trajectories, 1971662 timesteps found\n",
      "Average return: 3687.11, std: 875.27\n",
      "Max return: 5216.00, min: 1264.00\n",
      "Average length: 1572.30, std: 325.37\n",
      "Max length: 2000.00, min: 920.00\n",
      "==================================================\n",
      "Experiment log path: ./exp/2023.04.03/210426-default\n",
      "==================================================\n",
      "\n",
      "\n",
      "Making Eval Env.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "*** Pretrain ***\n",
      "----------------\n",
      "eval_envs: <stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7ff4a82f72e0>\n",
      "loss_fn: <function Experiment.__call__.<locals>.loss_fn at 0x7ff3b09e4700>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (1200) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(\u001b[38;5;28mvars\u001b[39m(args))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 449\u001b[0m, in \u001b[0;36mExperiment.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariant[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_pretrain_iters\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariant[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_online_iters\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMaking Online Env.....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 246\u001b[0m, in \u001b[0;36mExperiment.pretrain\u001b[0;34m(self, eval_envs, loss_fn)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_iter \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariant[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_pretrain_iters\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# in every iteration, prepare the data loader\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m create_dataloader(\n\u001b[1;32m    233\u001b[0m         trajectories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffline_trajs,\n\u001b[1;32m    234\u001b[0m         num_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariant[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_updates_per_pretrain_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         max_episode_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariant[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_episode_len\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     train_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     eval_outputs, eval_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(eval_fns)\n\u001b[1;32m    251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total\u001b[39m\u001b[38;5;124m\"\u001b[39m: time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time}\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/trainer.py:41\u001b[0m, in \u001b[0;36mSequenceTrainer.train_iteration\u001b[0;34m(self, loss_fn, dataloader)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, trajs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m---> 41\u001b[0m     loss, nll, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step_stochastic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrajs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     43\u001b[0m     nlls\u001b[38;5;241m.\u001b[39mappend(nll)\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/trainer.py:78\u001b[0m, in \u001b[0;36mSequenceTrainer.train_step_stochastic\u001b[0;34m(self, loss_fn, trajs)\u001b[0m\n\u001b[1;32m     74\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m padding_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     76\u001b[0m action_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclone(actions)\n\u001b[0;32m---> 78\u001b[0m _, action_preds, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrtg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mordering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#print(\"states : {}\".format(states))\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m#print(\"actions : {}\".format(actions))\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#print(\"rewards : {}\".format(rewards))\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#torch.save(action_preds, 'action_preds.pt')\u001b[39;00m\n\u001b[1;32m     92\u001b[0m loss, nll, entropy \u001b[38;5;241m=\u001b[39m loss_fn(\n\u001b[1;32m     93\u001b[0m     action_preds,  \u001b[38;5;66;03m# a_hat_dist\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     action_target,\n\u001b[1;32m     95\u001b[0m     padding_mask,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtemperature()\u001b[38;5;241m.\u001b[39mdetach(),  \u001b[38;5;66;03m# no gradient taken here\u001b[39;00m\n\u001b[1;32m     97\u001b[0m )\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/decision_transformer/models/decision_transformer.py:288\u001b[0m, in \u001b[0;36mDecisionTransformer.forward\u001b[0;34m(self, states, actions, rewards, returns_to_go, timesteps, ordering, padding_mask)\u001b[0m\n\u001b[1;32m    281\u001b[0m stacked_padding_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    282\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack((padding_mask, padding_mask, padding_mask), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m seq_length)\n\u001b[1;32m    285\u001b[0m )\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# we feed in the input embeddings (not word indices as in NLP) to the model\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacked_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacked_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m x \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m#print(\"x: {}\".format(x))\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m#print(\"stacked_inputs: {}\".format(stacked_inputs))\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m#print(\"stacked_padding_mask: {}\".format(stacked_padding_mask))\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# reshape x so that the second dimension corresponds to the original\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# returns (0), states (1), or actions (2); i.e. x[:,1,t] is the token for s_t\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/decision_transformer/models/trajectory_gpt2.py:733\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    723\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    724\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    725\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    730\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    731\u001b[0m     )\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 733\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m hidden_states, present \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/decision_transformer/models/trajectory_gpt2.py:303\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    294\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    302\u001b[0m ):\n\u001b[0;32m--> 303\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/decision_transformer/models/trajectory_gpt2.py:236\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     present \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,)\n\u001b[0;32m--> 236\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m a \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    239\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_heads(a)\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/decision_transformer/models/trajectory_gpt2.py:171\u001b[0m, in \u001b[0;36mAttention._attn\u001b[0;34m(self, q, k, v, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cross_attention:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# if only \"normal\" attention layer implements causal mask\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias[:, :, ns \u001b[38;5;241m-\u001b[39m nd: ns, :ns]\n\u001b[0;32m--> 171\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_bias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m+\u001b[39m attention_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (1200) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "utils.set_seed_everywhere(args.seed)\n",
    "experiment = Experiment(vars(args))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ae700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_env(env):\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    action_range = [\n",
    "        float(env.action_space.low.min()) + 1e-6,\n",
    "        float(env.action_space.high.max()) - 1e-6]\n",
    "        \n",
    "    print(\"state_dim: {}\".format(state_dim))\n",
    "    print(\"act_dim: {}\".format(act_dim))\n",
    "    print(\"action_range: {}\".format(action_range))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a72dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env = make_pytorch_env(args)\n",
    "their_env = gym.make('antmaze-large-diverse-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_env(my_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_env(their_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ffe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env.reset()\n",
    "my_env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234518ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70610f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "their_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "their_env.reset()\n",
    "their_env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac790187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment.variant\n",
    "#experiment.model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1b18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ec8a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2bbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c547220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e41b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.log(1e-310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db722b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds = torch.load('action_preds.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"a.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e15e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.log_likelihood(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b65b461",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sefude = action_preds.log_likelihood(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nan_to_num(sefude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f47b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc3b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3641331",
   "metadata": {},
   "outputs": [],
   "source": [
    "math.log(-0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07872499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.entropy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7aefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.log_likelihood(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "state_dim = 4\n",
    "hidden_size = 512\n",
    "\n",
    "embed_state = torch.nn.Linear(state_dim, hidden_size).to('cuda')\n",
    "embed_state_2 = torch.load('embed_state.pt').to('cuda')\n",
    "states = torch.load('states.pt').to('cuda')\n",
    "state_embeddings = embed_state(states)\n",
    "state_embeddings_2 = torch.load('state_embeddings.pt').to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92878d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e8be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"state_embeddings {}\".format(state_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26ca72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"state_embeddings 2 {}\".format(state_embeddings_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state_2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790f259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_state(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da41c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_state_2(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2bf17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737fe07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoppppppppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b433b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c390a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando as rewards pra ver se resolve o problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d99eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/drone_dataset.pkl', 'rb') as f:\n",
    "    my_data = pickle.load(f)\n",
    "    \n",
    "with open('data/antmaze-large-diverse-v2.pkl', 'rb') as f:\n",
    "    their_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in my_data:\n",
    "    rewards = data['actions']\n",
    "    print(\"max: {}\".format(np.max(rewards)))\n",
    "    print(\"min: {}\".format(np.min(rewards)))\n",
    "    print(\"mean: {}\".format(np.mean(rewards)))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(my_data[0]['observations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b79cc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.shape(their_data[0]['observations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308b8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7a8eb4bc",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/drone_dataset.pkl', 'rb') as f:\n",
    "    my_data = pickle.load(f)\n",
    "    \n",
    "\n",
    "for data in my_data:\n",
    "    \n",
    "    #data['rewards']   = np.float32(data['rewards'].flatten())\n",
    "    #data['terminals'] = np.float32(data['terminals'].flatten())\n",
    "    data['actions'] = np.float32(np.minimum(np.maximum(data['actions'], -1), 1))\n",
    "    #data['observations'] = np.float32(data['observations'])\n",
    "    #data['next_observations'] = np.float32(data['next_observations'])\n",
    "    \n",
    "with open('data/drone_dataset.pkl', 'wb') as handle:\n",
    "    pickle.dump(my_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96eb78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(v - v.min()) / (v.max() - v.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c58b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
