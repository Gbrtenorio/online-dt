{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7c8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbdcd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mv ../drone_dataset.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dcf0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade protobuf==3.20.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb98bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install transformers==4.5.1\n",
    "#!pip3 install -U tokenizers\n",
    "# The code below just solve many problems lol\n",
    "#!pip3 uninstall tokenizers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc16738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/CARLA_0.9.8/PythonAPI/carla/dist/carla-0.9.8-py3.5-linux-x86_64.egg/carla/libcarla.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "import d4rl\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from replay_buffer import ReplayBuffer\n",
    "from lamb import Lamb\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from pathlib import Path\n",
    "from data import create_dataloader\n",
    "from decision_transformer.models.decision_transformer import DecisionTransformer\n",
    "from evaluation import create_vec_eval_episodes_fn, vec_evaluate_episode_rtg\n",
    "from trainer import SequenceTrainer\n",
    "from logger import Logger\n",
    "\n",
    "from env import make_pytorch_env\n",
    "\n",
    "#MAX_EPISODE_LEN = 2000 # 4000 # 2000 # 4000 # Warning: there is a similar variable in data.py! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82242c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", type=int, default=10)\n",
    "parser.add_argument(\"--env\", type=str, default=\"drone_dataset\")\n",
    "\n",
    "# model options\n",
    "## Training Context Length K: (default: 20)\n",
    "parser.add_argument(\"--K\", type=int, default=40)\n",
    "## Embedding dimension: (default: 512)\n",
    "parser.add_argument(\"--embed_dim\", type=int, default=512)\n",
    "## Number of Layers: (default: 4)\n",
    "parser.add_argument(\"--n_layer\", type=int, default=8)\n",
    "## Number of Attention Heads: (default: 4)\n",
    "parser.add_argument(\"--n_head\", type=int, default=8)\n",
    "## Nonlinearity function: \n",
    "parser.add_argument(\"--activation_function\", type=str, default=\"relu\")\n",
    "## Dropout:\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "## Evaluating Context Length K: \n",
    "parser.add_argument(\"--eval_context_length\", type=int, default=10)\n",
    "## Positional embedding: absolute ordering\n",
    "parser.add_argument(\"--ordering\", type=int, default=1) # 0\n",
    "\n",
    "# shared evaluation options\n",
    "# g_eval: (default: 3600)\n",
    "parser.add_argument(\"--eval_rtg\", type=int, default=6000)\n",
    "parser.add_argument(\"--num_eval_episodes\", type=int, default=10)\n",
    "\n",
    "# shared training options\n",
    "parser.add_argument(\"--init_temperature\", type=float, default=0.1)\n",
    "## Batch Size: (default: 256)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "parser.add_argument(\"--learning_rate\", \"-lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--weight_decay\", \"-wd\", type=float, default=5e-4)\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=10000)\n",
    "\n",
    "# pretraining options\n",
    "parser.add_argument(\"--max_pretrain_iters\", type=int, default=1)\n",
    "parser.add_argument(\"--num_updates_per_pretrain_iter\", type=int, default=5000)\n",
    "\n",
    "# finetuning options\n",
    "parser.add_argument(\"--max_online_iters\", type=int, default=1500)\n",
    "parser.add_argument(\"--online_rtg\", type=int, default=7200)\n",
    "parser.add_argument(\"--num_online_rollouts\", type=int, default=1)\n",
    "parser.add_argument(\"--replay_size\", type=int, default=1000)\n",
    "parser.add_argument(\"--num_updates_per_online_iter\", type=int, default=300)\n",
    "parser.add_argument(\"--eval_interval\", type=int, default=10)\n",
    "\n",
    "# environment options\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "parser.add_argument(\"--log_to_tb\", \"-w\", type=bool, default=True)\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"./exp\")\n",
    "parser.add_argument(\"--exp_name\", type=str, default=\"default\")\n",
    "\n",
    "# general options\n",
    "parser.add_argument(\"--max_episode_len\", type=int, default=2000)\n",
    "\n",
    "# Add for fast Debbuging\n",
    "'''\n",
    "parser.add_argument(\"--K\", type=int, default=1)#####40)\n",
    "parser.add_argument(\"--embed_dim\", type=int, default=4)#####512)\n",
    "parser.add_argument(\"--n_layer\", type=int, default=2)#####8)\n",
    "parser.add_argument(\"--n_head\", type=int, default=2)#####8)\n",
    "parser.add_argument(\"--eval_context_length\", type=int, default=1)#####10) # d:5\n",
    "parser.add_argument(\"--eval_rtg\", type=int, default=600)#####6000)\n",
    "parser.add_argument(\"--num_eval_episodes\", type=int, default=2)#####10)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4)#####256)\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=10)#####10000)\n",
    "parser.add_argument(\"--num_updates_per_pretrain_iter\", type=int, default=500)#####5000)\n",
    "parser.add_argument(\"--max_online_iters\", type=int, default=500)#####1500)\n",
    "parser.add_argument(\"--online_rtg\", type=int, default=720)#####7200)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b539c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, variant):\n",
    "\n",
    "        self.state_dim, self.act_dim, self.action_range = self._get_env_spec(variant)\n",
    "        self.offline_trajs, self.state_mean, self.state_std = self._load_dataset(\n",
    "            variant[\"env\"]\n",
    "        )\n",
    "        # initialize by offline trajs\n",
    "        self.replay_buffer = ReplayBuffer(variant[\"replay_size\"], self.offline_trajs)\n",
    "\n",
    "        self.aug_trajs = []\n",
    "\n",
    "        self.device = variant.get(\"device\", \"cuda\")\n",
    "        self.target_entropy = -self.act_dim\n",
    "        self.model = DecisionTransformer(\n",
    "            state_dim=self.state_dim,\n",
    "            act_dim=self.act_dim,\n",
    "            action_range=self.action_range,\n",
    "            max_length=variant[\"K\"],\n",
    "            eval_context_length=variant[\"eval_context_length\"],\n",
    "            max_ep_len=variant[\"max_episode_len\"],\n",
    "            hidden_size=variant[\"embed_dim\"],\n",
    "            n_layer=variant[\"n_layer\"],\n",
    "            n_head=variant[\"n_head\"],\n",
    "            n_inner=4 * variant[\"embed_dim\"],\n",
    "            activation_function=variant[\"activation_function\"],\n",
    "            n_positions=1024,\n",
    "            resid_pdrop=variant[\"dropout\"],\n",
    "            attn_pdrop=variant[\"dropout\"],\n",
    "            stochastic_policy=True,\n",
    "            ordering=variant[\"ordering\"],\n",
    "            init_temperature=variant[\"init_temperature\"],\n",
    "            target_entropy=self.target_entropy,\n",
    "        ).to(device=self.device)\n",
    "\n",
    "        self.optimizer = Lamb(\n",
    "            self.model.parameters(),\n",
    "            lr=variant[\"learning_rate\"],\n",
    "            weight_decay=variant[\"weight_decay\"],\n",
    "            eps=1e-8,\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.optimizer, lambda steps: min((steps + 1) / variant[\"warmup_steps\"], 1)\n",
    "        )\n",
    "\n",
    "        self.log_temperature_optimizer = torch.optim.Adam(\n",
    "            [self.model.log_temperature],\n",
    "            lr=1e-4,\n",
    "            betas=[0.9, 0.999],\n",
    "        )\n",
    "\n",
    "        # track the training progress and\n",
    "        # training/evaluation/online performance in all the iterations\n",
    "        self.pretrain_iter = 0\n",
    "        self.online_iter = 0\n",
    "        self.total_transitions_sampled = 0\n",
    "        self.variant = variant\n",
    "        self.reward_scale = 1.0 if \"antmaze\" in variant[\"env\"] else 0.001\n",
    "        self.logger = Logger(variant)\n",
    "\n",
    "    def _get_env_spec(self, variant):\n",
    "        #####env = gym.make(variant[\"env\"])\n",
    "        env = make_pytorch_env(args)\n",
    "        #env.max_step = MAX_EPISODE_LEN\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.shape[0]\n",
    "        #action_range = [-0.999999, 0.999999]\n",
    "        \n",
    "        action_range = [\n",
    "            float(env.action_space.low.min()) + 1e-6,\n",
    "            float(env.action_space.high.max()) - 1e-6,\n",
    "        ]\n",
    "        \n",
    "        print(\"action_range: {}\".format(action_range))\n",
    "        env.close()\n",
    "        return state_dim, act_dim, action_range\n",
    "\n",
    "    def _save_model(self, path_prefix, is_pretrain_model=False):\n",
    "        to_save = {\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": self.scheduler.state_dict(),\n",
    "            \"pretrain_iter\": self.pretrain_iter,\n",
    "            \"online_iter\": self.online_iter,\n",
    "            \"args\": self.variant,\n",
    "            \"total_transitions_sampled\": self.total_transitions_sampled,\n",
    "            \"np\": np.random.get_state(),\n",
    "            \"python\": random.getstate(),\n",
    "            \"pytorch\": torch.get_rng_state(),\n",
    "            \"log_temperature_optimizer_state_dict\": self.log_temperature_optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        with open(f\"{path_prefix}/model.pt\", \"wb\") as f:\n",
    "            torch.save(to_save, f)\n",
    "        print(f\"\\nModel saved at {path_prefix}/model.pt\")\n",
    "\n",
    "        if is_pretrain_model:\n",
    "            with open(f\"{path_prefix}/pretrain_model.pt\", \"wb\") as f:\n",
    "                torch.save(to_save, f)\n",
    "            print(f\"Model saved at {path_prefix}/pretrain_model.pt\")\n",
    "\n",
    "    def _load_model(self, path_prefix):\n",
    "        if Path(f\"{path_prefix}/model.pt\").exists():\n",
    "            with open(f\"{path_prefix}/model.pt\", \"rb\") as f:\n",
    "                checkpoint = torch.load(f)\n",
    "            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "            self.log_temperature_optimizer.load_state_dict(\n",
    "                checkpoint[\"log_temperature_optimizer_state_dict\"]\n",
    "            )\n",
    "            self.pretrain_iter = checkpoint[\"pretrain_iter\"]\n",
    "            self.online_iter = checkpoint[\"online_iter\"]\n",
    "            self.total_transitions_sampled = checkpoint[\"total_transitions_sampled\"]\n",
    "            np.random.set_state(checkpoint[\"np\"])\n",
    "            random.setstate(checkpoint[\"python\"])\n",
    "            torch.set_rng_state(checkpoint[\"pytorch\"])\n",
    "            print(f\"Model loaded at {path_prefix}/model.pt\")\n",
    "\n",
    "    def _load_dataset(self, env_name):\n",
    "\n",
    "        dataset_path = f\"./data/{env_name}.pkl\"\n",
    "        with open(dataset_path, \"rb\") as f:\n",
    "            trajectories = pickle.load(f)\n",
    "\n",
    "        states, traj_lens, returns = [], [], []\n",
    "        for path in trajectories:\n",
    "            states.append(path[\"observations\"])\n",
    "            traj_lens.append(len(path[\"observations\"]))\n",
    "            returns.append(path[\"rewards\"].sum())\n",
    "        traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "        # used for input normalization\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        num_timesteps = sum(traj_lens)\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Starting new experiment: {env_name}\")\n",
    "        print(f\"{len(traj_lens)} trajectories, {num_timesteps} timesteps found\")\n",
    "        print(f\"Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}\")\n",
    "        print(f\"Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}\")\n",
    "        print(f\"Average length: {np.mean(traj_lens):.2f}, std: {np.std(traj_lens):.2f}\")\n",
    "        print(f\"Max length: {np.max(traj_lens):.2f}, min: {np.min(traj_lens):.2f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "        num_trajectories = 1\n",
    "        timesteps = traj_lens[sorted_inds[-1]]\n",
    "        ind = len(trajectories) - 2\n",
    "        while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] < num_timesteps:\n",
    "            timesteps += traj_lens[sorted_inds[ind]]\n",
    "            num_trajectories += 1\n",
    "            ind -= 1\n",
    "        sorted_inds = sorted_inds[-num_trajectories:]\n",
    "        trajectories = [trajectories[ii] for ii in sorted_inds]\n",
    "\n",
    "        return trajectories, state_mean, state_std\n",
    "\n",
    "    def _augment_trajectories(\n",
    "        self,\n",
    "        online_envs,\n",
    "        target_explore,\n",
    "        n,\n",
    "        randomized=False,\n",
    "    ):\n",
    "\n",
    "        max_ep_len = self.variant[\"max_episode_len\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generate init state\n",
    "            target_return = [target_explore * self.reward_scale] * online_envs.num_envs\n",
    "\n",
    "            returns, lengths, trajs = vec_evaluate_episode_rtg(\n",
    "                online_envs,\n",
    "                self.state_dim,\n",
    "                self.act_dim,\n",
    "                self.model,\n",
    "                max_ep_len=max_ep_len,\n",
    "                reward_scale=self.reward_scale,\n",
    "                target_return=target_return,\n",
    "                mode=\"normal\",\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                device=self.device,\n",
    "                use_mean=False,\n",
    "            )\n",
    "\n",
    "        self.replay_buffer.add_new_trajs(trajs)\n",
    "        self.aug_trajs += trajs\n",
    "        self.total_transitions_sampled += np.sum(lengths)\n",
    "\n",
    "        return {\n",
    "            \"aug_traj/return\": np.mean(returns),\n",
    "            \"aug_traj/length\": np.mean(lengths),\n",
    "        }\n",
    "\n",
    "    def pretrain(self, eval_envs, loss_fn):\n",
    "        print(\"\\n\\n\\n*** Pretrain ***\")\n",
    "        print(\"----------------\")\n",
    "        print(\"eval_envs: {}\".format(eval_envs))\n",
    "        print(\"loss_fn: {}\".format(loss_fn))\n",
    "        \n",
    "        eval_fns = [\n",
    "            create_vec_eval_episodes_fn(\n",
    "                vec_env=eval_envs,\n",
    "                eval_rtg=self.variant[\"eval_rtg\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                device=self.device,\n",
    "                use_mean=True,\n",
    "                reward_scale=self.reward_scale,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        trainer = SequenceTrainer(\n",
    "            model=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            log_temperature_optimizer=self.log_temperature_optimizer,\n",
    "            scheduler=self.scheduler,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        writer = (\n",
    "            SummaryWriter(self.logger.log_path) if self.variant[\"log_to_tb\"] else None\n",
    "        )\n",
    "        while self.pretrain_iter < self.variant[\"max_pretrain_iters\"]:\n",
    "            # in every iteration, prepare the data loader\n",
    "            dataloader = create_dataloader(\n",
    "                trajectories=self.offline_trajs,\n",
    "                num_iters=self.variant[\"num_updates_per_pretrain_iter\"],\n",
    "                batch_size=self.variant[\"batch_size\"],\n",
    "                max_len=self.variant[\"K\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                reward_scale=self.reward_scale,\n",
    "                action_range=self.action_range,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "\n",
    "            train_outputs = trainer.train_iteration(\n",
    "                loss_fn=loss_fn,\n",
    "                dataloader=dataloader,\n",
    "            )\n",
    "            eval_outputs, eval_reward = self.evaluate(eval_fns)\n",
    "            outputs = {\"time/total\": time.time() - self.start_time}\n",
    "            outputs.update(train_outputs)\n",
    "            outputs.update(eval_outputs)\n",
    "            self.logger.log_metrics(\n",
    "                outputs,\n",
    "                iter_num=self.pretrain_iter,\n",
    "                total_transitions_sampled=self.total_transitions_sampled,\n",
    "                writer=writer,\n",
    "            )\n",
    "\n",
    "            self._save_model(\n",
    "                path_prefix=self.logger.log_path,\n",
    "                is_pretrain_model=True,\n",
    "            )\n",
    "\n",
    "            self.pretrain_iter += 1\n",
    "\n",
    "    def evaluate(self, eval_fns):\n",
    "        eval_start = time.time()\n",
    "        self.model.eval()\n",
    "        outputs = {}\n",
    "        for eval_fn in eval_fns:\n",
    "            o = eval_fn(self.model)\n",
    "            outputs.update(o)\n",
    "        outputs[\"time/evaluation\"] = time.time() - eval_start\n",
    "\n",
    "        eval_reward = outputs[\"evaluation/return_mean_gm\"]\n",
    "        return outputs, eval_reward\n",
    "\n",
    "    def online_tuning(self, online_envs, eval_envs, loss_fn):\n",
    "\n",
    "        print(\"\\n\\n\\n*** Online Finetuning ***\")\n",
    "\n",
    "        trainer = SequenceTrainer(\n",
    "            model=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            log_temperature_optimizer=self.log_temperature_optimizer,\n",
    "            scheduler=self.scheduler,\n",
    "            device=self.device,\n",
    "        )\n",
    "        eval_fns = [\n",
    "            create_vec_eval_episodes_fn(\n",
    "                vec_env=eval_envs,\n",
    "                eval_rtg=self.variant[\"eval_rtg\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                device=self.device,\n",
    "                use_mean=True,\n",
    "                reward_scale=self.reward_scale,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "        ]\n",
    "        writer = (\n",
    "            SummaryWriter(self.logger.log_path) if self.variant[\"log_to_tb\"] else None\n",
    "        )\n",
    "        while self.online_iter < self.variant[\"max_online_iters\"]:\n",
    "\n",
    "            outputs = {}\n",
    "            augment_outputs = self._augment_trajectories(\n",
    "                online_envs,\n",
    "                self.variant[\"online_rtg\"],\n",
    "                n=self.variant[\"num_online_rollouts\"],\n",
    "            )\n",
    "            outputs.update(augment_outputs)\n",
    "\n",
    "            dataloader = create_dataloader(\n",
    "                trajectories=self.replay_buffer.trajectories,\n",
    "                num_iters=self.variant[\"num_updates_per_online_iter\"],\n",
    "                batch_size=self.variant[\"batch_size\"],\n",
    "                max_len=self.variant[\"K\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                reward_scale=self.reward_scale,\n",
    "                action_range=self.action_range,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "\n",
    "            # finetuning\n",
    "            is_last_iter = self.online_iter == self.variant[\"max_online_iters\"] - 1\n",
    "            if (self.online_iter + 1) % self.variant[\n",
    "                \"eval_interval\"\n",
    "            ] == 0 or is_last_iter:\n",
    "                evaluation = True\n",
    "            else:\n",
    "                evaluation = False\n",
    "\n",
    "            train_outputs = trainer.train_iteration(\n",
    "                loss_fn=loss_fn,\n",
    "                dataloader=dataloader,\n",
    "            )\n",
    "            outputs.update(train_outputs)\n",
    "\n",
    "            if evaluation:\n",
    "                eval_outputs, eval_reward = self.evaluate(eval_fns)\n",
    "                outputs.update(eval_outputs)\n",
    "\n",
    "            outputs[\"time/total\"] = time.time() - self.start_time\n",
    "\n",
    "            # log the metrics\n",
    "            self.logger.log_metrics(\n",
    "                outputs,\n",
    "                iter_num=self.pretrain_iter + self.online_iter,\n",
    "                total_transitions_sampled=self.total_transitions_sampled,\n",
    "                writer=writer,\n",
    "            )\n",
    "\n",
    "            self._save_model(\n",
    "                path_prefix=self.logger.log_path,\n",
    "                is_pretrain_model=False,\n",
    "            )\n",
    "\n",
    "            self.online_iter += 1\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        utils.set_seed_everywhere(args.seed)\n",
    "\n",
    "        import d4rl\n",
    "\n",
    "        def loss_fn(\n",
    "            a_hat_dist,     # action_preds\n",
    "            a,              # action_target\n",
    "            attention_mask, # padding_mask\n",
    "            entropy_reg,    # self.model.temperature().detach()\n",
    "        ):\n",
    "            # a_hat is a SquashedNormal Distribution\n",
    "            log_likelihood = a_hat_dist.log_likelihood(a)[attention_mask > 0].mean()\n",
    "            \n",
    "            entropy = a_hat_dist.entropy().mean()\n",
    "            loss = -(log_likelihood + entropy_reg * entropy)\n",
    "            \n",
    "            '''\n",
    "            print(\"a_hat_dist : {}\".format(a_hat_dist))\n",
    "            print(\"a : {}\".format(a))\n",
    "            torch.save(a,\"a.pt\")\n",
    "            print(\"a_hat_dist.log_likelihood(a) : {}\".format(a_hat_dist.log_likelihood(a)))\n",
    "            #print(\"attention_mask : {}\".format(attention_mask))\n",
    "            print(\"log_likelihood: {}\".format(log_likelihood))\n",
    "            print(\"loss inside jupyter: {} of type: {}\".format(loss,type(loss)))\n",
    "            '''\n",
    "            \n",
    "            return (\n",
    "                loss,\n",
    "                -log_likelihood,\n",
    "                entropy,\n",
    "            )\n",
    "\n",
    "        def get_env_builder(seed, env_name, target_goal=None):\n",
    "            def make_env_fn():\n",
    "                import d4rl\n",
    "\n",
    "                #####env = gym.make(env_name)\n",
    "                env = make_pytorch_env(args)\n",
    "                #env.max_step = MAX_EPISODE_LEN\n",
    "                env.seed(seed)\n",
    "                '''\n",
    "                if hasattr(env.env, \"wrapped_env\"):\n",
    "                    env.env.wrapped_env.seed(seed)\n",
    "                elif hasattr(env.env, \"seed\"):\n",
    "                    env.env.seed(seed)\n",
    "                else:\n",
    "                    pass\n",
    "                '''\n",
    "                '''\n",
    "                env.action_space.seed(seed)\n",
    "                env.observation_space.seed(seed)\n",
    "                '''\n",
    "\n",
    "                if target_goal:\n",
    "                    env.set_target_goal(target_goal)\n",
    "                    print(f\"Set the target goal to be {env.target_goal}\")\n",
    "                return env\n",
    "\n",
    "            return make_env_fn\n",
    "\n",
    "        print(\"\\n\\nMaking Eval Env.....\")\n",
    "        env_name = self.variant[\"env\"]\n",
    "        if \"antmaze\" in env_name:\n",
    "            env = gym.make(env_name)\n",
    "            target_goal = env.target_goal\n",
    "            env.close()\n",
    "            print(f\"Generated the fixed target goal: {target_goal}\")\n",
    "        else:\n",
    "            target_goal = None\n",
    "        eval_envs = SubprocVecEnv(\n",
    "            [\n",
    "                get_env_builder(i, env_name=env_name, target_goal=target_goal)\n",
    "                for i in range(self.variant[\"num_eval_episodes\"])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        if self.variant[\"max_pretrain_iters\"]:\n",
    "            self.pretrain(eval_envs, loss_fn)\n",
    "        \n",
    "        if self.variant[\"max_online_iters\"]:\n",
    "            print(\"\\n\\nMaking Online Env.....\")\n",
    "            online_envs = SubprocVecEnv(\n",
    "                [\n",
    "                    get_env_builder(i + 100, env_name=env_name, target_goal=target_goal)\n",
    "                    for i in range(self.variant[\"num_online_rollouts\"])\n",
    "                ]\n",
    "            )\n",
    "            self.online_tuning(online_envs, eval_envs, loss_fn)\n",
    "            online_envs.close()\n",
    "\n",
    "        eval_envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428b67f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/odt/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_range: [-0.999999, 0.999999]\n",
      "==================================================\n",
      "Starting new experiment: drone_dataset\n",
      "1254 trajectories, 1971662 timesteps found\n",
      "Average return: 3687.11, std: 875.27\n",
      "Max return: 5216.00, min: 1264.00\n",
      "Average length: 1572.30, std: 325.37\n",
      "Max length: 2000.00, min: 920.00\n",
      "==================================================\n",
      "Experiment log path: ./exp/2023.03.23/210846-default\n",
      "==================================================\n",
      "\n",
      "\n",
      "Making Eval Env.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n",
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "*** Pretrain ***\n",
      "----------------\n",
      "eval_envs: <stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fb12b024f70>\n",
      "loss_fn: <function Experiment.__call__.<locals>.loss_fn at 0x7fb12af4c550>\n",
      "================================================================================\n",
      "Iteration 0\n",
      "time/total: 2114.2444989681244\n",
      "time/training: 2100.520931482315\n",
      "training/train_loss_mean: 485.36548055641714\n",
      "training/train_loss_std: 1814.8151574660537\n",
      "training/nll: -8.106392860412598\n",
      "training/entropy: -5.479274272918701\n",
      "training/temp_value: 0.1389832622826801\n",
      "evaluation/return_mean_gm: -15252.124855877486\n",
      "evaluation/return_std_gm: 585.554763952337\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.651319026947021\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "Model saved at ./exp/2023.03.23/210846-default/pretrain_model.pt\n",
      "\n",
      "\n",
      "Making Online Env.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "*** Online Finetuning ***\n",
      "================================================================================\n",
      "Iteration 1\n",
      "aug_traj/return: -9673.539614860325\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.313720703125\n",
      "training/train_loss_mean: -7.490225026782818\n",
      "training/train_loss_std: 0.32555692744405623\n",
      "training/nll: -8.513853073120117\n",
      "training/entropy: -5.705856800079346\n",
      "training/temp_value: 0.14412743401064995\n",
      "time/total: 2250.48313164711\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 2\n",
      "aug_traj/return: -10534.549647993284\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.218674659729\n",
      "training/train_loss_mean: -7.625753367341819\n",
      "training/train_loss_std: 0.3218401629551256\n",
      "training/nll: -8.470282554626465\n",
      "training/entropy: -5.619393825531006\n",
      "training/temp_value: 0.14955678086315907\n",
      "time/total: 2383.7400357723236\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 3\n",
      "aug_traj/return: -15603.031585460478\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.3924560546875\n",
      "training/train_loss_mean: -7.731088190730953\n",
      "training/train_loss_std: 0.3713613177595729\n",
      "training/nll: -9.1443510055542\n",
      "training/entropy: -6.182422161102295\n",
      "training/temp_value: 0.1552689279865915\n",
      "time/total: 2517.2178699970245\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 4\n",
      "aug_traj/return: -9895.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.470782995224\n",
      "training/train_loss_mean: -7.708047728383767\n",
      "training/train_loss_std: 0.4954679967433021\n",
      "training/nll: -8.454716682434082\n",
      "training/entropy: -5.832648754119873\n",
      "training/temp_value: 0.16106450403379496\n",
      "time/total: 2651.222713947296\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 5\n",
      "aug_traj/return: -14073.033268478915\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.35551047325134\n",
      "training/train_loss_mean: -7.795593215377481\n",
      "training/train_loss_std: 0.42187217898609913\n",
      "training/nll: -8.978477478027344\n",
      "training/entropy: -6.034940242767334\n",
      "training/temp_value: 0.16703258399584053\n",
      "time/total: 2785.3202373981476\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 6\n",
      "aug_traj/return: -15841.332324607141\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.41110634803772\n",
      "training/train_loss_mean: -7.848833702907056\n",
      "training/train_loss_std: 0.4278664228990767\n",
      "training/nll: -9.349737167358398\n",
      "training/entropy: -6.207805633544922\n",
      "training/temp_value: 0.17307569987330848\n",
      "time/total: 2918.8221130371094\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 7\n",
      "aug_traj/return: -15243.998824278398\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.36339807510376\n",
      "training/train_loss_mean: -7.860466294218674\n",
      "training/train_loss_std: 0.45869675713245794\n",
      "training/nll: -9.284163475036621\n",
      "training/entropy: -6.649524211883545\n",
      "training/temp_value: 0.17934860516792037\n",
      "time/total: 3053.313513278961\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 8\n",
      "aug_traj/return: -15805.123462504103\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.52467608451843\n",
      "training/train_loss_mean: -7.980058476893834\n",
      "training/train_loss_std: 0.37748377494912916\n",
      "training/nll: -9.863401412963867\n",
      "training/entropy: -6.602349758148193\n",
      "training/temp_value: 0.18585754003681249\n",
      "time/total: 3187.2453503608704\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 9\n",
      "aug_traj/return: -15429.061782018018\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.60494542121887\n",
      "training/train_loss_mean: -7.957153169933838\n",
      "training/train_loss_std: 0.5259852342961999\n",
      "training/nll: -9.44146728515625\n",
      "training/entropy: -6.304274559020996\n",
      "training/temp_value: 0.19256469585073738\n",
      "time/total: 3320.9379074573517\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 10\n",
      "aug_traj/return: -18529.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.10477137565613\n",
      "training/train_loss_mean: -7.991600383543397\n",
      "training/train_loss_std: 0.47057200771627117\n",
      "training/nll: -9.897587776184082\n",
      "training/entropy: -6.572271823883057\n",
      "training/temp_value: 0.1994041389811419\n",
      "evaluation/return_mean_gm: -22805.20264840689\n",
      "evaluation/return_std_gm: 0.4052968137766583\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.59311580657959\n",
      "time/total: 3468.145895719528\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 11\n",
      "aug_traj/return: -19609.115689481856\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.70319128036499\n",
      "training/train_loss_mean: -8.199021327671542\n",
      "training/train_loss_std: 0.38892976321865486\n",
      "training/nll: -9.822393417358398\n",
      "training/entropy: -6.90497350692749\n",
      "training/temp_value: 0.20676001703983243\n",
      "time/total: 3601.927626848221\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 12\n",
      "aug_traj/return: -11453.300529736323\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.22343730926514\n",
      "training/train_loss_mean: -8.209334912862271\n",
      "training/train_loss_std: 0.43598389512095437\n",
      "training/nll: -10.234966278076172\n",
      "training/entropy: -6.930069923400879\n",
      "training/temp_value: 0.21430465999792747\n",
      "time/total: 3735.4828777313232\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 13\n",
      "aug_traj/return: -16113.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.77744269371033\n",
      "training/train_loss_mean: -8.248517451362623\n",
      "training/train_loss_std: 0.42794339019431366\n",
      "training/nll: -9.593873977661133\n",
      "training/entropy: -7.02795934677124\n",
      "training/temp_value: 0.22200520672260338\n",
      "time/total: 3869.403887271881\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 14\n",
      "aug_traj/return: -15390.447685633875\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.52128291130066\n",
      "training/train_loss_mean: -8.159586956095042\n",
      "training/train_loss_std: 0.5853930811282857\n",
      "training/nll: -10.033944129943848\n",
      "training/entropy: -6.549030303955078\n",
      "training/temp_value: 0.2298438548153001\n",
      "time/total: 4003.381083250046\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 15\n",
      "aug_traj/return: -14618.431031628439\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.11250138282776\n",
      "training/train_loss_mean: -8.062310611283886\n",
      "training/train_loss_std: 0.46109359020600704\n",
      "training/nll: -10.223176956176758\n",
      "training/entropy: -7.111227512359619\n",
      "training/temp_value: 0.23736622407361185\n",
      "time/total: 4136.809484243393\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 16\n",
      "aug_traj/return: -18376.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.36749649047852\n",
      "training/train_loss_mean: -8.130032064120202\n",
      "training/train_loss_std: 0.6963151738605654\n",
      "training/nll: -9.32614517211914\n",
      "training/entropy: -6.718501567840576\n",
      "training/temp_value: 0.24579317838310824\n",
      "time/total: 4270.026541471481\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 17\n",
      "aug_traj/return: -21071.214354977525\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.23875331878662\n",
      "training/train_loss_mean: -8.059889352208865\n",
      "training/train_loss_std: 0.6616569505639093\n",
      "training/nll: -9.227420806884766\n",
      "training/entropy: -6.803053379058838\n",
      "training/temp_value: 0.2541659495816097\n",
      "time/total: 4403.69810461998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 18\n",
      "aug_traj/return: -7944.006460521699\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.60695505142212\n",
      "training/train_loss_mean: -8.442174642551599\n",
      "training/train_loss_std: 0.4070900462555881\n",
      "training/nll: -9.66983699798584\n",
      "training/entropy: -6.842126369476318\n",
      "training/temp_value: 0.2632891111194778\n",
      "time/total: 4537.902723789215\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 19\n",
      "aug_traj/return: -10793.501987656271\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.60411190986633\n",
      "training/train_loss_mean: -8.529940348801723\n",
      "training/train_loss_std: 0.403202286107585\n",
      "training/nll: -9.390022277832031\n",
      "training/entropy: -7.028358459472656\n",
      "training/temp_value: 0.2728224064807252\n",
      "time/total: 4671.672616958618\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 20\n",
      "aug_traj/return: -4164.831370037468\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.27671146392822\n",
      "training/train_loss_mean: -8.421928213902673\n",
      "training/train_loss_std: 0.4635791318236942\n",
      "training/nll: -10.434625625610352\n",
      "training/entropy: -7.144837379455566\n",
      "training/temp_value: 0.2823662794840397\n",
      "evaluation/return_mean_gm: -22462.023492228156\n",
      "evaluation/return_std_gm: 11.9530155436878\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.566593885421753\n",
      "time/total: 4819.071261644363\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 21\n",
      "aug_traj/return: -16768.2519529527\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.37553930282593\n",
      "training/train_loss_mean: -8.454087924168494\n",
      "training/train_loss_std: 0.39147909769072026\n",
      "training/nll: -10.543434143066406\n",
      "training/entropy: -7.012777805328369\n",
      "training/temp_value: 0.2921260226672149\n",
      "time/total: 4952.911214590073\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 22\n",
      "aug_traj/return: -18056.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.3188726902008\n",
      "training/train_loss_mean: -8.435171272333575\n",
      "training/train_loss_std: 0.41893365759218504\n",
      "training/nll: -10.510749816894531\n",
      "training/entropy: -7.088527202606201\n",
      "training/temp_value: 0.3022078698191546\n",
      "time/total: 5086.605586528778\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 23\n",
      "aug_traj/return: -10147.190625107753\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.16833901405334\n",
      "training/train_loss_mean: -8.62204190548459\n",
      "training/train_loss_std: 0.3777616682243974\n",
      "training/nll: -11.1232328414917\n",
      "training/entropy: -7.299574375152588\n",
      "training/temp_value: 0.3128380141468583\n",
      "time/total: 5220.1101334095\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 24\n",
      "aug_traj/return: -17412.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.40685677528381\n",
      "training/train_loss_mean: -8.632540859013208\n",
      "training/train_loss_std: 0.40319683199437145\n",
      "training/nll: -11.374354362487793\n",
      "training/entropy: -7.372503757476807\n",
      "training/temp_value: 0.32374155542797783\n",
      "time/total: 5353.798196077347\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 25\n",
      "aug_traj/return: -7938.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 128.03842639923096\n",
      "training/train_loss_mean: -8.571688482752812\n",
      "training/train_loss_std: 0.38136331305653326\n",
      "training/nll: -10.46786117553711\n",
      "training/entropy: -6.971360683441162\n",
      "training/temp_value: 0.33484179022430594\n",
      "time/total: 5487.731643676758\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 26\n",
      "aug_traj/return: -16903.81382653162\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.33103680610657\n",
      "training/train_loss_mean: -8.718758994117023\n",
      "training/train_loss_std: 0.29587238865896603\n",
      "training/nll: -11.409049034118652\n",
      "training/entropy: -7.259862422943115\n",
      "training/temp_value: 0.3464741962728986\n",
      "time/total: 5620.962288856506\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 27\n",
      "aug_traj/return: -15000.519603759056\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.27108931541443\n",
      "training/train_loss_mean: -8.583378271553281\n",
      "training/train_loss_std: 0.35893265259432405\n",
      "training/nll: -11.247164726257324\n",
      "training/entropy: -7.175384521484375\n",
      "training/temp_value: 0.3583035907242522\n",
      "time/total: 5754.520971536636\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 28\n",
      "aug_traj/return: -2362.1525110318125\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.59912657737732\n",
      "training/train_loss_mean: -8.621278533516461\n",
      "training/train_loss_std: 0.3392080376634465\n",
      "training/nll: -11.81855583190918\n",
      "training/entropy: -7.4187445640563965\n",
      "training/temp_value: 0.3704082028447925\n",
      "time/total: 5888.38484454155\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 29\n",
      "aug_traj/return: -1979.8209135791258\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.33183288574219\n",
      "training/train_loss_mean: -8.589309670612552\n",
      "training/train_loss_std: 0.32313142347897833\n",
      "training/nll: -11.460121154785156\n",
      "training/entropy: -7.299351692199707\n",
      "training/temp_value: 0.3829941919628204\n",
      "time/total: 6022.210832595825\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 30\n",
      "aug_traj/return: -8947.024820696479\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.26446413993835\n",
      "training/train_loss_mean: -8.618977022137257\n",
      "training/train_loss_std: 0.3181886481698437\n",
      "training/nll: -11.638344764709473\n",
      "training/entropy: -7.443063259124756\n",
      "training/temp_value: 0.3961729543714477\n",
      "evaluation/return_mean_gm: -15018.556708835935\n",
      "evaluation/return_std_gm: 76.16041654317044\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.702053546905518\n",
      "time/total: 6170.046132564545\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 31\n",
      "aug_traj/return: -7411.067955756379\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.63665103912354\n",
      "training/train_loss_mean: -8.617875067175527\n",
      "training/train_loss_std: 0.283252127947513\n",
      "training/nll: -11.84222412109375\n",
      "training/entropy: -7.502632141113281\n",
      "training/temp_value: 0.4097960454281915\n",
      "time/total: 6303.76948595047\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 32\n",
      "aug_traj/return: -8075.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.54309844970703\n",
      "training/train_loss_mean: -8.489159004299717\n",
      "training/train_loss_std: 0.335246889847243\n",
      "training/nll: -11.31745719909668\n",
      "training/entropy: -7.335919380187988\n",
      "training/temp_value: 0.4236745420316037\n",
      "time/total: 6437.54559135437\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 33\n",
      "aug_traj/return: -8320.928330708284\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.25653910636902\n",
      "training/train_loss_mean: -8.528777956049957\n",
      "training/train_loss_std: 0.27198509950376015\n",
      "training/nll: -11.83041763305664\n",
      "training/entropy: -7.577686309814453\n",
      "training/temp_value: 0.4382268495819089\n",
      "time/total: 6570.9539206027985\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 34\n",
      "aug_traj/return: -5369.247881234052\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.41977214813232\n",
      "training/train_loss_mean: -8.466748753904561\n",
      "training/train_loss_std: 0.2518364633526745\n",
      "training/nll: -11.687350273132324\n",
      "training/entropy: -7.2762451171875\n",
      "training/temp_value: 0.45322589808023295\n",
      "time/total: 6704.3006727695465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 35\n",
      "aug_traj/return: -4802.6055832046895\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.31180047988892\n",
      "training/train_loss_mean: -8.394217494437163\n",
      "training/train_loss_std: 0.2459819692985502\n",
      "training/nll: -11.72803783416748\n",
      "training/entropy: -7.603883266448975\n",
      "training/temp_value: 0.46874407402065593\n",
      "time/total: 6837.724826812744\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 36\n",
      "aug_traj/return: -2184.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.38120484352112\n",
      "training/train_loss_mean: -8.247947189966846\n",
      "training/train_loss_std: 0.3297887148231673\n",
      "training/nll: -11.420433044433594\n",
      "training/entropy: -7.193024635314941\n",
      "training/temp_value: 0.48454824662970125\n",
      "time/total: 6971.376536369324\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 37\n",
      "aug_traj/return: -11877.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.40717267990112\n",
      "training/train_loss_mean: -8.153495439495861\n",
      "training/train_loss_std: 0.27661446600887774\n",
      "training/nll: -11.689309120178223\n",
      "training/entropy: -7.306732177734375\n",
      "training/temp_value: 0.5008144589911001\n",
      "time/total: 7105.478456258774\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 38\n",
      "aug_traj/return: -800.0773900996361\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.1612901687622\n",
      "training/train_loss_mean: -8.05818776709073\n",
      "training/train_loss_std: 0.3038432651901143\n",
      "training/nll: -11.532708168029785\n",
      "training/entropy: -7.2737321853637695\n",
      "training/temp_value: 0.5176200885983536\n",
      "time/total: 7238.538387060165\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 39\n",
      "aug_traj/return: -11009.501819507597\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.50982236862183\n",
      "training/train_loss_mean: -7.958595463864831\n",
      "training/train_loss_std: 0.2558885161052426\n",
      "training/nll: -12.124964714050293\n",
      "training/entropy: -7.382411956787109\n",
      "training/temp_value: 0.5349572193722018\n",
      "time/total: 7372.222225189209\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 40\n",
      "aug_traj/return: -3370.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.62492060661316\n",
      "training/train_loss_mean: -7.877950101786749\n",
      "training/train_loss_std: 0.2572781329512997\n",
      "training/nll: -11.746749877929688\n",
      "training/entropy: -7.277682781219482\n",
      "training/temp_value: 0.5529369448438031\n",
      "evaluation/return_mean_gm: -12053.704912223464\n",
      "evaluation/return_std_gm: 1590.0711441098117\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.542653799057007\n",
      "time/total: 7519.750795841217\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 41\n",
      "aug_traj/return: -3018.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.83909821510315\n",
      "training/train_loss_mean: -7.764089405690526\n",
      "training/train_loss_std: 0.2598788086055623\n",
      "training/nll: -11.737098693847656\n",
      "training/entropy: -7.329848766326904\n",
      "training/temp_value: 0.5714709130836008\n",
      "time/total: 7653.8458251953125\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 42\n",
      "aug_traj/return: -4050.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.50474119186401\n",
      "training/train_loss_mean: -7.6760516635100915\n",
      "training/train_loss_std: 0.2442126531895719\n",
      "training/nll: -12.357733726501465\n",
      "training/entropy: -7.5306620597839355\n",
      "training/temp_value: 0.5905990551509891\n",
      "time/total: 7787.545751810074\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 43\n",
      "aug_traj/return: -2241.900868173895\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.39017677307129\n",
      "training/train_loss_mean: -7.533420915200256\n",
      "training/train_loss_std: 0.23703266809961698\n",
      "training/nll: -11.691338539123535\n",
      "training/entropy: -7.2364044189453125\n",
      "training/temp_value: 0.6102720920373932\n",
      "time/total: 7920.902728557587\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 44\n",
      "aug_traj/return: -1901.8774634554238\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.61571097373962\n",
      "training/train_loss_mean: -7.400459503753853\n",
      "training/train_loss_std: 0.2461218896242694\n",
      "training/nll: -11.94644832611084\n",
      "training/entropy: -7.275143623352051\n",
      "training/temp_value: 0.630624768260467\n",
      "time/total: 8054.884435415268\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 45\n",
      "aug_traj/return: -4261.663370882757\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.6024706363678\n",
      "training/train_loss_mean: -7.253911595634605\n",
      "training/train_loss_std: 0.26314793871879044\n",
      "training/nll: -11.700435638427734\n",
      "training/entropy: -7.223255157470703\n",
      "training/temp_value: 0.6516019526221075\n",
      "time/total: 8188.376846551895\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 46\n",
      "aug_traj/return: -8227.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.28385901451111\n",
      "training/train_loss_mean: -7.150400333012212\n",
      "training/train_loss_std: 0.23015897808543573\n",
      "training/nll: -12.298834800720215\n",
      "training/entropy: -7.420260906219482\n",
      "training/temp_value: 0.6732561712905678\n",
      "time/total: 8321.64022397995\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 47\n",
      "aug_traj/return: -3100.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.52891039848328\n",
      "training/train_loss_mean: -6.996607359471515\n",
      "training/train_loss_std: 0.22387025566292013\n",
      "training/nll: -11.884994506835938\n",
      "training/entropy: -7.262219429016113\n",
      "training/temp_value: 0.6955279794933957\n",
      "time/total: 8455.294315814972\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 48\n",
      "aug_traj/return: -1986.6329493337312\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.33548331260681\n",
      "training/train_loss_mean: -6.885180996301293\n",
      "training/train_loss_std: 0.2262611148998353\n",
      "training/nll: -12.34194278717041\n",
      "training/entropy: -7.316693305969238\n",
      "training/temp_value: 0.7186581797563873\n",
      "time/total: 8588.47631764412\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 49\n",
      "aug_traj/return: -1704.9869353146591\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.82411456108093\n",
      "training/train_loss_mean: -6.696815103731051\n",
      "training/train_loss_std: 0.22748872916306248\n",
      "training/nll: -12.162951469421387\n",
      "training/entropy: -7.188842296600342\n",
      "training/temp_value: 0.742446333624308\n",
      "time/total: 8722.553020238876\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 50\n",
      "aug_traj/return: -1626.1741873071412\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.1366674900055\n",
      "training/train_loss_mean: -6.554093016762194\n",
      "training/train_loss_std: 0.22332010072554093\n",
      "training/nll: -11.996686935424805\n",
      "training/entropy: -7.09862756729126\n",
      "training/temp_value: 0.7669206814548992\n",
      "evaluation/return_mean_gm: -9463.653575641963\n",
      "evaluation/return_std_gm: 531.5430940999355\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.586916446685791\n",
      "time/total: 8869.528981685638\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 51\n",
      "aug_traj/return: -5588.915511880038\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.5868673324585\n",
      "training/train_loss_mean: -6.391397925932484\n",
      "training/train_loss_std: 0.2297568804254345\n",
      "training/nll: -11.64619255065918\n",
      "training/entropy: -7.116620063781738\n",
      "training/temp_value: 0.7921938240363517\n",
      "time/total: 9003.318449497223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 52\n",
      "aug_traj/return: -3812.0955484475235\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.30602240562439\n",
      "training/train_loss_mean: -6.228546736154475\n",
      "training/train_loss_std: 0.21938057907267305\n",
      "training/nll: -12.045866966247559\n",
      "training/entropy: -7.189733982086182\n",
      "training/temp_value: 0.8182223768161956\n",
      "time/total: 9136.809084415436\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 53\n",
      "aug_traj/return: -1287.8470630222532\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.27637982368469\n",
      "training/train_loss_mean: -6.051904610214445\n",
      "training/train_loss_std: 0.21089151126631148\n",
      "training/nll: -11.760930061340332\n",
      "training/entropy: -7.032107830047607\n",
      "training/temp_value: 0.8450565677472541\n",
      "time/total: 9270.206394672394\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 54\n",
      "aug_traj/return: -2357.0496051112173\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.52279496192932\n",
      "training/train_loss_mean: -5.877430327750453\n",
      "training/train_loss_std: 0.2032539893140311\n",
      "training/nll: -11.998845100402832\n",
      "training/entropy: -6.99141788482666\n",
      "training/temp_value: 0.8726292980527054\n",
      "time/total: 9404.22711634636\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 55\n",
      "aug_traj/return: -7789.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.82617163658142\n",
      "training/train_loss_mean: -5.692941641589118\n",
      "training/train_loss_std: 0.21117307284502523\n",
      "training/nll: -11.521685600280762\n",
      "training/entropy: -6.761359691619873\n",
      "training/temp_value: 0.9009436941664555\n",
      "time/total: 9538.174023389816\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 56\n",
      "aug_traj/return: -10041.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.39903521537781\n",
      "training/train_loss_mean: -5.502903991788446\n",
      "training/train_loss_std: 0.19206792882399495\n",
      "training/nll: -11.44800853729248\n",
      "training/entropy: -6.659597873687744\n",
      "training/temp_value: 0.9299494872367252\n",
      "time/total: 9671.5182762146\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 57\n",
      "aug_traj/return: -7201.373948151652\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.33109450340271\n",
      "training/train_loss_mean: -5.3167723352714455\n",
      "training/train_loss_std: 0.20278131093367557\n",
      "training/nll: -11.203670501708984\n",
      "training/entropy: -6.524824619293213\n",
      "training/temp_value: 0.9594935526026781\n",
      "time/total: 9804.838196277618\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 58\n",
      "aug_traj/return: -139.07863695850676\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.5376832485199\n",
      "training/train_loss_mean: -5.152538417655138\n",
      "training/train_loss_std: 0.19854269397969743\n",
      "training/nll: -11.331901550292969\n",
      "training/entropy: -6.35986852645874\n",
      "training/temp_value: 0.9891884309282838\n",
      "time/total: 9938.73786354065\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 59\n",
      "aug_traj/return: -2514.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.3826494216919\n",
      "training/train_loss_mean: -4.98200110741128\n",
      "training/train_loss_std: 0.18845051094867837\n",
      "training/nll: -10.351659774780273\n",
      "training/entropy: -5.5126566886901855\n",
      "training/temp_value: 1.0172809001780416\n",
      "time/total: 10072.58505487442\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 60\n",
      "aug_traj/return: -80.25115490066321\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.49729061126709\n",
      "training/train_loss_mean: -4.908079207032344\n",
      "training/train_loss_std: 0.1822935821299249\n",
      "training/nll: -8.163715362548828\n",
      "training/entropy: -2.8085412979125977\n",
      "training/temp_value: 1.0290866711721045\n",
      "evaluation/return_mean_gm: -12426.895439073716\n",
      "evaluation/return_std_gm: 406.44826393573874\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.578795671463013\n",
      "time/total: 10220.095966815948\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 61\n",
      "aug_traj/return: -3953.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.20001006126404\n",
      "training/train_loss_mean: -4.905826620550303\n",
      "training/train_loss_std: 0.17082005364545425\n",
      "training/nll: -8.262818336486816\n",
      "training/entropy: -3.101539373397827\n",
      "training/temp_value: 1.0272678079245574\n",
      "time/total: 10353.341439008713\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 62\n",
      "aug_traj/return: -2422.519258981087\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.3249089717865\n",
      "training/train_loss_mean: -4.93032425869176\n",
      "training/train_loss_std: 0.17790000271151835\n",
      "training/nll: -8.15182113647461\n",
      "training/entropy: -2.9255287647247314\n",
      "training/temp_value: 1.025468199593458\n",
      "time/total: 10486.846764326096\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 63\n",
      "aug_traj/return: -1488.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.32399535179138\n",
      "training/train_loss_mean: -4.945947775133112\n",
      "training/train_loss_std: 0.1851319492772454\n",
      "training/nll: -7.691431045532227\n",
      "training/entropy: -2.82784366607666\n",
      "training/temp_value: 1.024036120996945\n",
      "time/total: 10620.482833862305\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 64\n",
      "aug_traj/return: -3786.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 128.2568395137787\n",
      "training/train_loss_mean: -4.952249243956153\n",
      "training/train_loss_std: 0.18195121273912326\n",
      "training/nll: -8.051392555236816\n",
      "training/entropy: -3.081343412399292\n",
      "training/temp_value: 1.023605047580162\n",
      "time/total: 10754.897224187851\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 65\n",
      "aug_traj/return: -3631.0801119168113\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.50995874404907\n",
      "training/train_loss_mean: -4.960972278823361\n",
      "training/train_loss_std: 0.17910868708049046\n",
      "training/nll: -8.306832313537598\n",
      "training/entropy: -3.1339879035949707\n",
      "training/temp_value: 1.0225488155338636\n",
      "time/total: 10888.886750221252\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 66\n",
      "aug_traj/return: -1800.4830370835732\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.62400579452515\n",
      "training/train_loss_mean: -4.95787222625076\n",
      "training/train_loss_std: 0.1823874535568546\n",
      "training/nll: -8.717232704162598\n",
      "training/entropy: -3.2037923336029053\n",
      "training/temp_value: 1.023030722786618\n",
      "time/total: 11023.321414709091\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 67\n",
      "aug_traj/return: -2505.9120654944772\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.3922176361084\n",
      "training/train_loss_mean: -4.973541655117441\n",
      "training/train_loss_std: 0.1878576426426981\n",
      "training/nll: -7.806552886962891\n",
      "training/entropy: -2.9047083854675293\n",
      "training/temp_value: 1.0227864043759598\n",
      "time/total: 11157.26090168953\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 68\n",
      "aug_traj/return: -1139.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.36137676239014\n",
      "training/train_loss_mean: -4.966608712738705\n",
      "training/train_loss_std: 0.16875679510446018\n",
      "training/nll: -7.700723171234131\n",
      "training/entropy: -2.8398873805999756\n",
      "training/temp_value: 1.021979093674857\n",
      "time/total: 11291.017843008041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 69\n",
      "aug_traj/return: -2228.2708357778765\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.32409358024597\n",
      "training/train_loss_mean: -4.964005753189904\n",
      "training/train_loss_std: 0.18049886563508927\n",
      "training/nll: -8.59914779663086\n",
      "training/entropy: -3.041093587875366\n",
      "training/temp_value: 1.0219139553505876\n",
      "time/total: 11425.222650527954\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 70\n",
      "aug_traj/return: -2642.773358190107\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.55595254898071\n",
      "training/train_loss_mean: -4.955961346290846\n",
      "training/train_loss_std: 0.167536105855215\n",
      "training/nll: -7.830422401428223\n",
      "training/entropy: -2.9493188858032227\n",
      "training/temp_value: 1.021584024005158\n",
      "evaluation/return_mean_gm: -7452.656333461555\n",
      "evaluation/return_std_gm: 2908.188075092388\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.616621971130371\n",
      "time/total: 11572.666473150253\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 71\n",
      "aug_traj/return: -3601.803028845348\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.48659777641296\n",
      "training/train_loss_mean: -4.957430699955863\n",
      "training/train_loss_std: 0.178108797550899\n",
      "training/nll: -8.219426155090332\n",
      "training/entropy: -3.017974615097046\n",
      "training/temp_value: 1.0215467015242594\n",
      "time/total: 11706.32228755951\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 72\n",
      "aug_traj/return: -2648.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.39351439476013\n",
      "training/train_loss_mean: -4.973882337737902\n",
      "training/train_loss_std: 0.18000712367248806\n",
      "training/nll: -7.812431335449219\n",
      "training/entropy: -2.919219732284546\n",
      "training/temp_value: 1.0221090899838534\n",
      "time/total: 11839.831867456436\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 73\n",
      "aug_traj/return: 275.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.01798701286316\n",
      "training/train_loss_mean: -4.97561412796219\n",
      "training/train_loss_std: 0.1869246695695843\n",
      "training/nll: -8.118369102478027\n",
      "training/entropy: -3.1524577140808105\n",
      "training/temp_value: 1.021568450408511\n",
      "time/total: 11973.070446968079\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 74\n",
      "aug_traj/return: -2692.888236710386\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.14513111114502\n",
      "training/train_loss_mean: -4.977350162730549\n",
      "training/train_loss_std: 0.18321441352873288\n",
      "training/nll: -7.650493621826172\n",
      "training/entropy: -2.801043748855591\n",
      "training/temp_value: 1.0213675563419293\n",
      "time/total: 12106.902854919434\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 75\n",
      "aug_traj/return: -1638.8936498372723\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.31952428817749\n",
      "training/train_loss_mean: -4.964307548456318\n",
      "training/train_loss_std: 0.18564341470103926\n",
      "training/nll: -8.01677417755127\n",
      "training/entropy: -2.8889942169189453\n",
      "training/temp_value: 1.0213763610716313\n",
      "time/total: 12240.51825261116\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 76\n",
      "aug_traj/return: -3597.8387279865315\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.15518569946289\n",
      "training/train_loss_mean: -4.959568382440264\n",
      "training/train_loss_std: 0.17733999768885092\n",
      "training/nll: -7.949580669403076\n",
      "training/entropy: -2.9362776279449463\n",
      "training/temp_value: 1.0224446196848191\n",
      "time/total: 12373.802732229233\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 77\n",
      "aug_traj/return: -1837.0262549478707\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.67000246047974\n",
      "training/train_loss_mean: -4.959091204093196\n",
      "training/train_loss_std: 0.18375787254020795\n",
      "training/nll: -8.189021110534668\n",
      "training/entropy: -3.271061420440674\n",
      "training/temp_value: 1.0212295009348382\n",
      "time/total: 12507.678905963898\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 78\n",
      "aug_traj/return: -3521.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.53075194358826\n",
      "training/train_loss_mean: -4.957787138805992\n",
      "training/train_loss_std: 0.17803021840056335\n",
      "training/nll: -7.918284893035889\n",
      "training/entropy: -2.968343734741211\n",
      "training/temp_value: 1.019832901157436\n",
      "time/total: 12642.087476015091\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 79\n",
      "aug_traj/return: -4082.2900247206194\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.42200088500977\n",
      "training/train_loss_mean: -4.962518975573925\n",
      "training/train_loss_std: 0.2051448019643356\n",
      "training/nll: -8.84127140045166\n",
      "training/entropy: -3.389542818069458\n",
      "training/temp_value: 1.0216778880810857\n",
      "time/total: 12775.56962609291\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 80\n",
      "aug_traj/return: -3681.1892332065045\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.3375871181488\n",
      "training/train_loss_mean: -4.95129870844583\n",
      "training/train_loss_std: 0.18074814631945146\n",
      "training/nll: -8.082268714904785\n",
      "training/entropy: -3.0940663814544678\n",
      "training/temp_value: 1.0219290938521868\n",
      "evaluation/return_mean_gm: -8960.144481296807\n",
      "evaluation/return_std_gm: 2062.4108365682255\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.592765092849731\n",
      "time/total: 12922.51920056343\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 81\n",
      "aug_traj/return: -2427.831816715845\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.16063570976257\n",
      "training/train_loss_mean: -4.9613558041519585\n",
      "training/train_loss_std: 0.17562153080125015\n",
      "training/nll: -7.880540370941162\n",
      "training/entropy: -2.9260709285736084\n",
      "training/temp_value: 1.0213398813890902\n",
      "time/total: 13056.314139127731\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 82\n",
      "aug_traj/return: -2847.3224675697047\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.21253657341003\n",
      "training/train_loss_mean: -4.938208988071989\n",
      "training/train_loss_std: 0.17959077453093675\n",
      "training/nll: -7.568504333496094\n",
      "training/entropy: -2.6719977855682373\n",
      "training/temp_value: 1.0224702862038535\n",
      "time/total: 13189.790227413177\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 83\n",
      "aug_traj/return: 435.50542995915\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.37282872200012\n",
      "training/train_loss_mean: -4.958187079262277\n",
      "training/train_loss_std: 0.1739372501867048\n",
      "training/nll: -8.108630180358887\n",
      "training/entropy: -3.08905029296875\n",
      "training/temp_value: 1.0206535939898882\n",
      "time/total: 13323.3172519207\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 84\n",
      "aug_traj/return: -2478.736367472996\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.38277959823608\n",
      "training/train_loss_mean: -4.955091297716306\n",
      "training/train_loss_std: 0.17387457114272553\n",
      "training/nll: -7.964108467102051\n",
      "training/entropy: -3.1161789894104004\n",
      "training/temp_value: 1.02015221183346\n",
      "time/total: 13456.850798130035\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 85\n",
      "aug_traj/return: -2157.5982515377973\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.39342999458313\n",
      "training/train_loss_mean: -4.934628204138678\n",
      "training/train_loss_std: 0.17206199965224203\n",
      "training/nll: -8.339094161987305\n",
      "training/entropy: -3.260850191116333\n",
      "training/temp_value: 1.0212962456700072\n",
      "time/total: 13590.510927438736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 86\n",
      "aug_traj/return: -2321.6655872848937\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.21845245361328\n",
      "training/train_loss_mean: -4.950611316902603\n",
      "training/train_loss_std: 0.17987583099972462\n",
      "training/nll: -8.133801460266113\n",
      "training/entropy: -2.921909809112549\n",
      "training/temp_value: 1.023179537840018\n",
      "time/total: 13723.692293167114\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 87\n",
      "aug_traj/return: -2052.4071153669124\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.33053803443909\n",
      "training/train_loss_mean: -4.932280317543361\n",
      "training/train_loss_std: 0.16781275725991118\n",
      "training/nll: -8.006542205810547\n",
      "training/entropy: -3.1798818111419678\n",
      "training/temp_value: 1.0221022617544655\n",
      "time/total: 13857.107142686844\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 88\n",
      "aug_traj/return: -595.954878035112\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.07156562805176\n",
      "training/train_loss_mean: -4.930051889037512\n",
      "training/train_loss_std: 0.19011941768930876\n",
      "training/nll: -8.126880645751953\n",
      "training/entropy: -3.142216920852661\n",
      "training/temp_value: 1.0217678608233647\n",
      "time/total: 13990.281732559204\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 89\n",
      "aug_traj/return: -5797.697104432167\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.09309411048889\n",
      "training/train_loss_mean: -4.9256455062794355\n",
      "training/train_loss_std: 0.17986749948578762\n",
      "training/nll: -8.0733060836792\n",
      "training/entropy: -3.1122167110443115\n",
      "training/temp_value: 1.0187240280344434\n",
      "time/total: 14123.621178150177\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 90\n",
      "aug_traj/return: -1349.2944028114593\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.12342047691345\n",
      "training/train_loss_mean: -4.9341919828549585\n",
      "training/train_loss_std: 0.1778652072967481\n",
      "training/nll: -7.890115737915039\n",
      "training/entropy: -2.9583685398101807\n",
      "training/temp_value: 1.0202861448938958\n",
      "evaluation/return_mean_gm: -7940.252573819729\n",
      "evaluation/return_std_gm: 528.7585368309442\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.467438459396362\n",
      "time/total: 14270.164543151855\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 91\n",
      "aug_traj/return: -3863.2463265836395\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.39540314674377\n",
      "training/train_loss_mean: -4.9160741517420945\n",
      "training/train_loss_std: 0.177377885785832\n",
      "training/nll: -8.193586349487305\n",
      "training/entropy: -3.1301076412200928\n",
      "training/temp_value: 1.0200631417727704\n",
      "time/total: 14404.718150377274\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 92\n",
      "aug_traj/return: -718.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.20458221435547\n",
      "training/train_loss_mean: -4.916789338203601\n",
      "training/train_loss_std: 0.18788244309030638\n",
      "training/nll: -8.166582107543945\n",
      "training/entropy: -3.2952721118927\n",
      "training/temp_value: 1.0198636491573585\n",
      "time/total: 14538.42878818512\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 93\n",
      "aug_traj/return: -3809.2563373117073\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.15700912475586\n",
      "training/train_loss_mean: -4.937048781106447\n",
      "training/train_loss_std: 0.1866229564903441\n",
      "training/nll: -8.1260986328125\n",
      "training/entropy: -3.3270671367645264\n",
      "training/temp_value: 1.0228580003848757\n",
      "time/total: 14672.10043001175\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 94\n",
      "aug_traj/return: -1825.5910056149605\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.41490006446838\n",
      "training/train_loss_mean: -4.922459262673429\n",
      "training/train_loss_std: 0.18465874087887352\n",
      "training/nll: -8.364792823791504\n",
      "training/entropy: -3.2011592388153076\n",
      "training/temp_value: 1.021205456275787\n",
      "time/total: 14805.918172836304\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 95\n",
      "aug_traj/return: -2734.2695226178957\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.21807551383972\n",
      "training/train_loss_mean: -4.9137106270443685\n",
      "training/train_loss_std: 0.17649685987683203\n",
      "training/nll: -8.10770320892334\n",
      "training/entropy: -2.8269221782684326\n",
      "training/temp_value: 1.0227829279303928\n",
      "time/total: 14939.07488656044\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 96\n",
      "aug_traj/return: -483.91365168157705\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.32162094116211\n",
      "training/train_loss_mean: -4.92449033931508\n",
      "training/train_loss_std: 0.1868623936176097\n",
      "training/nll: -7.997302532196045\n",
      "training/entropy: -3.0719287395477295\n",
      "training/temp_value: 1.0199617869265545\n",
      "time/total: 15072.63131403923\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 97\n",
      "aug_traj/return: -1776.1981687472944\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.54170799255371\n",
      "training/train_loss_mean: -4.899055460638557\n",
      "training/train_loss_std: 0.18555964495892732\n",
      "training/nll: -8.291197776794434\n",
      "training/entropy: -3.3424839973449707\n",
      "training/temp_value: 1.0186255666402437\n",
      "time/total: 15206.418411970139\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 98\n",
      "aug_traj/return: -2105.5679711254456\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.5045166015625\n",
      "training/train_loss_mean: -4.923447624219285\n",
      "training/train_loss_std: 0.17983128927927372\n",
      "training/nll: -8.051458358764648\n",
      "training/entropy: -2.937633752822876\n",
      "training/temp_value: 1.020526973964679\n",
      "time/total: 15340.27256822586\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 99\n",
      "aug_traj/return: -1906.8359640277336\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.43381834030151\n",
      "training/train_loss_mean: -4.9138949305896995\n",
      "training/train_loss_std: 0.17894276569388426\n",
      "training/nll: -8.910074234008789\n",
      "training/entropy: -3.4270806312561035\n",
      "training/temp_value: 1.0214389871627165\n",
      "time/total: 15473.907857656479\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 100\n",
      "aug_traj/return: -2377.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.09466791152954\n",
      "training/train_loss_mean: -4.887927449895698\n",
      "training/train_loss_std: 0.18451564612319135\n",
      "training/nll: -7.866335391998291\n",
      "training/entropy: -2.9523489475250244\n",
      "training/temp_value: 1.0203526013383253\n",
      "evaluation/return_mean_gm: -4749.704245817858\n",
      "evaluation/return_std_gm: 2298.3960031191036\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.575085401535034\n",
      "time/total: 15620.695757627487\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 101\n",
      "aug_traj/return: -2210.6608886144577\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.25247406959534\n",
      "training/train_loss_mean: -4.9011850864977635\n",
      "training/train_loss_std: 0.19006039770078362\n",
      "training/nll: -7.5545878410339355\n",
      "training/entropy: -2.8137385845184326\n",
      "training/temp_value: 1.0187932688291568\n",
      "time/total: 15754.409878015518\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 102\n",
      "aug_traj/return: -1937.5493046355596\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.152822971344\n",
      "training/train_loss_mean: -4.8944711145647775\n",
      "training/train_loss_std: 0.17394713878589163\n",
      "training/nll: -7.7532806396484375\n",
      "training/entropy: -2.855113983154297\n",
      "training/temp_value: 1.0216473378414088\n",
      "time/total: 15887.511022567749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 103\n",
      "aug_traj/return: -4077.488006092503\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.37723660469055\n",
      "training/train_loss_mean: -4.888995764348415\n",
      "training/train_loss_std: 0.17870473136631906\n",
      "training/nll: -7.891871452331543\n",
      "training/entropy: -3.0671756267547607\n",
      "training/temp_value: 1.0204512413930622\n",
      "time/total: 16020.960854768753\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 104\n",
      "aug_traj/return: -3055.4539291886585\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.56203937530518\n",
      "training/train_loss_mean: -4.8884461689611864\n",
      "training/train_loss_std: 0.18468386787160052\n",
      "training/nll: -7.673949718475342\n",
      "training/entropy: -2.7084896564483643\n",
      "training/temp_value: 1.0197298892582949\n",
      "time/total: 16154.52244234085\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 105\n",
      "aug_traj/return: -20.071887450380018\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.39182662963867\n",
      "training/train_loss_mean: -4.869510331817593\n",
      "training/train_loss_std: 0.18994823070768416\n",
      "training/nll: -7.933788299560547\n",
      "training/entropy: -3.083042621612549\n",
      "training/temp_value: 1.018468939695321\n",
      "time/total: 16288.160460233688\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 106\n",
      "aug_traj/return: -1278.136702667648\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.57963514328003\n",
      "training/train_loss_mean: -4.878546028829222\n",
      "training/train_loss_std: 0.18222028060870477\n",
      "training/nll: -7.4524383544921875\n",
      "training/entropy: -2.8842613697052\n",
      "training/temp_value: 1.0194514051140877\n",
      "time/total: 16422.462133407593\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 107\n",
      "aug_traj/return: -2082.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.43371653556824\n",
      "training/train_loss_mean: -4.8700028233069625\n",
      "training/train_loss_std: 0.179575263959155\n",
      "training/nll: -7.67739200592041\n",
      "training/entropy: -2.923445224761963\n",
      "training/temp_value: 1.0220743677982949\n",
      "time/total: 16556.671756505966\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 108\n",
      "aug_traj/return: -2339.492360780062\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.08661890029907\n",
      "training/train_loss_mean: -4.855727810313785\n",
      "training/train_loss_std: 0.20277214021633566\n",
      "training/nll: -7.690035343170166\n",
      "training/entropy: -2.879399061203003\n",
      "training/temp_value: 1.0194608152123719\n",
      "time/total: 16690.074191093445\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 109\n",
      "aug_traj/return: -1548.126713983321\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.12495517730713\n",
      "training/train_loss_mean: -4.875679120810346\n",
      "training/train_loss_std: 0.181827143520688\n",
      "training/nll: -8.106303215026855\n",
      "training/entropy: -3.16164493560791\n",
      "training/temp_value: 1.0218281261066946\n",
      "time/total: 16823.530966758728\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 110\n",
      "aug_traj/return: -1507.2421228965038\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.17348885536194\n",
      "training/train_loss_mean: -4.853497564050102\n",
      "training/train_loss_std: 0.18648913845839374\n",
      "training/nll: -7.305239677429199\n",
      "training/entropy: -2.671802282333374\n",
      "training/temp_value: 1.024013043690629\n",
      "evaluation/return_mean_gm: -228.12024965164719\n",
      "evaluation/return_std_gm: 1396.418323925645\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.602622032165527\n",
      "time/total: 16970.387483119965\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 111\n",
      "aug_traj/return: -1978.1919076289043\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.19228768348694\n",
      "training/train_loss_mean: -4.845056735900928\n",
      "training/train_loss_std: 0.18716403978837726\n",
      "training/nll: -7.647967338562012\n",
      "training/entropy: -2.9965484142303467\n",
      "training/temp_value: 1.0191510395036982\n",
      "time/total: 17103.871332645416\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 112\n",
      "aug_traj/return: -1101.4710147028895\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.11051321029663\n",
      "training/train_loss_mean: -4.847329147823644\n",
      "training/train_loss_std: 0.19510705769731912\n",
      "training/nll: -8.307576179504395\n",
      "training/entropy: -3.202969789505005\n",
      "training/temp_value: 1.0208378197117447\n",
      "time/total: 17237.12024664879\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 113\n",
      "aug_traj/return: -3473.233399844572\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.02942180633545\n",
      "training/train_loss_mean: -4.861192730453326\n",
      "training/train_loss_std: 0.18613075943665922\n",
      "training/nll: -7.955767631530762\n",
      "training/entropy: -3.043329954147339\n",
      "training/temp_value: 1.0198962121135777\n",
      "time/total: 17370.42845582962\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 114\n",
      "aug_traj/return: -3300.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.16975378990173\n",
      "training/train_loss_mean: -4.839754441079988\n",
      "training/train_loss_std: 0.17586978487261376\n",
      "training/nll: -7.925910472869873\n",
      "training/entropy: -2.946268081665039\n",
      "training/temp_value: 1.01998356887453\n",
      "time/total: 17503.576832056046\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 115\n",
      "aug_traj/return: -1392.0834332796535\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.2757956981659\n",
      "training/train_loss_mean: -4.839403707887023\n",
      "training/train_loss_std: 0.18290607275892945\n",
      "training/nll: -7.91089391708374\n",
      "training/entropy: -3.1065127849578857\n",
      "training/temp_value: 1.0200192885734825\n",
      "time/total: 17637.483828783035\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 116\n",
      "aug_traj/return: -2761.365969192115\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.32901310920715\n",
      "training/train_loss_mean: -4.826021827873492\n",
      "training/train_loss_std: 0.19929990995947558\n",
      "training/nll: -7.167568206787109\n",
      "training/entropy: -2.759979009628296\n",
      "training/temp_value: 1.017586768165149\n",
      "time/total: 17771.106063365936\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 117\n",
      "aug_traj/return: -2708.840278992613\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.23151922225952\n",
      "training/train_loss_mean: -4.832397872001995\n",
      "training/train_loss_std: 0.18897427324142876\n",
      "training/nll: -8.081535339355469\n",
      "training/entropy: -3.0512402057647705\n",
      "training/temp_value: 1.0205838189222391\n",
      "time/total: 17904.5949382782\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 118\n",
      "aug_traj/return: -1160.873400742699\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.36307168006897\n",
      "training/train_loss_mean: -4.834434054004963\n",
      "training/train_loss_std: 0.18630221842487205\n",
      "training/nll: -7.122262954711914\n",
      "training/entropy: -2.7203731536865234\n",
      "training/temp_value: 1.0194359887216016\n",
      "time/total: 18038.330185890198\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 119\n",
      "aug_traj/return: -1713.2464013104961\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.3914909362793\n",
      "training/train_loss_mean: -4.829579124814053\n",
      "training/train_loss_std: 0.1824701834103259\n",
      "training/nll: -8.090667724609375\n",
      "training/entropy: -3.186612844467163\n",
      "training/temp_value: 1.0206240590288325\n",
      "time/total: 18172.648675441742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 120\n",
      "aug_traj/return: -132.14730108438744\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.9992618560791\n",
      "training/train_loss_mean: -4.80662362074915\n",
      "training/train_loss_std: 0.18500895245234972\n",
      "training/nll: -7.059218406677246\n",
      "training/entropy: -2.6377193927764893\n",
      "training/temp_value: 1.0211542773438012\n",
      "evaluation/return_mean_gm: -5973.018261334088\n",
      "evaluation/return_std_gm: 1268.9433281010117\n",
      "evaluation/length_mean_gm: 2000.0\n",
      "evaluation/length_std_gm: 0.0\n",
      "time/evaluation: 13.589059591293335\n",
      "time/total: 18320.227265119553\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 121\n",
      "aug_traj/return: -100.61943347653687\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.19037985801697\n",
      "training/train_loss_mean: -4.8312761826772626\n",
      "training/train_loss_std: 0.18534708746246845\n",
      "training/nll: -8.059770584106445\n",
      "training/entropy: -3.0569374561309814\n",
      "training/temp_value: 1.0185109095188078\n",
      "time/total: 18453.49631500244\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 122\n",
      "aug_traj/return: -2737.5292482361942\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.25175762176514\n",
      "training/train_loss_mean: -4.815779237906438\n",
      "training/train_loss_std: 0.1817729064717146\n",
      "training/nll: -7.776421070098877\n",
      "training/entropy: -3.0068304538726807\n",
      "training/temp_value: 1.020296563899816\n",
      "time/total: 18587.06591773033\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 123\n",
      "aug_traj/return: -3310.9619346485138\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.31518483161926\n",
      "training/train_loss_mean: -4.802412828211431\n",
      "training/train_loss_std: 0.1858381330737016\n",
      "training/nll: -8.115260124206543\n",
      "training/entropy: -3.0078539848327637\n",
      "training/temp_value: 1.020869567998262\n",
      "time/total: 18720.682085752487\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n",
      "================================================================================\n",
      "Iteration 124\n",
      "aug_traj/return: -1256.0\n",
      "aug_traj/length: 2000.0\n",
      "time/training: 127.11183261871338\n",
      "training/train_loss_mean: -4.791431717758854\n",
      "training/train_loss_std: 0.17915982221869556\n",
      "training/nll: -7.944567680358887\n",
      "training/entropy: -3.0900492668151855\n",
      "training/temp_value: 1.0176835971222338\n",
      "time/total: 18854.068633317947\n",
      "\n",
      "Model saved at ./exp/2023.03.23/210846-default/model.pt\n"
     ]
    }
   ],
   "source": [
    "utils.set_seed_everywhere(args.seed)\n",
    "experiment = Experiment(vars(args))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ae700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_env(env):\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    action_range = [\n",
    "        float(env.action_space.low.min()) + 1e-6,\n",
    "        float(env.action_space.high.max()) - 1e-6]\n",
    "        \n",
    "    print(\"state_dim: {}\".format(state_dim))\n",
    "    print(\"act_dim: {}\".format(act_dim))\n",
    "    print(\"action_range: {}\".format(action_range))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a72dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env = make_pytorch_env(args)\n",
    "their_env = gym.make('antmaze-large-diverse-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_env(my_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_env(their_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ffe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env.reset()\n",
    "my_env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234518ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70610f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "their_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "their_env.reset()\n",
    "their_env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac790187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment.variant\n",
    "#experiment.model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1b18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ec8a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2bbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c547220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e41b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.log(1e-310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db722b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds = torch.load('action_preds.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"a.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e15e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.log_likelihood(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b65b461",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sefude = action_preds.log_likelihood(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nan_to_num(sefude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f47b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc3b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3641331",
   "metadata": {},
   "outputs": [],
   "source": [
    "math.log(-0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07872499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.entropy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7aefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.log_likelihood(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "state_dim = 4\n",
    "hidden_size = 512\n",
    "\n",
    "embed_state = torch.nn.Linear(state_dim, hidden_size).to('cuda')\n",
    "embed_state_2 = torch.load('embed_state.pt').to('cuda')\n",
    "states = torch.load('states.pt').to('cuda')\n",
    "state_embeddings = embed_state(states)\n",
    "state_embeddings_2 = torch.load('state_embeddings.pt').to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92878d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e8be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"state_embeddings {}\".format(state_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26ca72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"state_embeddings 2 {}\".format(state_embeddings_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state_2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790f259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_state(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da41c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_state_2(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2bf17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737fe07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoppppppppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b433b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c390a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando as rewards pra ver se resolve o problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d99eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/drone_dataset.pkl', 'rb') as f:\n",
    "    my_data = pickle.load(f)\n",
    "    \n",
    "with open('data/antmaze-large-diverse-v2.pkl', 'rb') as f:\n",
    "    their_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in my_data:\n",
    "    rewards = data['actions']\n",
    "    print(\"max: {}\".format(np.max(rewards)))\n",
    "    print(\"min: {}\".format(np.min(rewards)))\n",
    "    print(\"mean: {}\".format(np.mean(rewards)))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(my_data[0]['observations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b79cc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.shape(their_data[0]['observations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308b8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7a8eb4bc",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/drone_dataset.pkl', 'rb') as f:\n",
    "    my_data = pickle.load(f)\n",
    "    \n",
    "\n",
    "for data in my_data:\n",
    "    \n",
    "    #data['rewards']   = np.float32(data['rewards'].flatten())\n",
    "    #data['terminals'] = np.float32(data['terminals'].flatten())\n",
    "    data['actions'] = np.float32(np.minimum(np.maximum(data['actions'], -1), 1))\n",
    "    #data['observations'] = np.float32(data['observations'])\n",
    "    #data['next_observations'] = np.float32(data['next_observations'])\n",
    "    \n",
    "with open('data/drone_dataset.pkl', 'wb') as handle:\n",
    "    pickle.dump(my_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96eb78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(v - v.min()) / (v.max() - v.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c58b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
