{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdcd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mv ../drone_dataset.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade protobuf==3.20.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install transformers==4.5.1\n",
    "#!pip3 install -U tokenizers\n",
    "# The code below just solve many problems lol\n",
    "#!pip3 uninstall tokenizers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc16738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "import d4rl\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from replay_buffer import ReplayBuffer\n",
    "from lamb import Lamb\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from pathlib import Path\n",
    "from data import create_dataloader\n",
    "from decision_transformer.models.decision_transformer import DecisionTransformer\n",
    "from evaluation import create_vec_eval_episodes_fn, vec_evaluate_episode_rtg\n",
    "from trainer import SequenceTrainer\n",
    "from logger import Logger\n",
    "\n",
    "from env import make_pytorch_env\n",
    "\n",
    "#MAX_EPISODE_LEN = 2000 # 4000 # 2000 # 4000 # Warning: there is a similar variable in data.py! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", type=int, default=10)\n",
    "parser.add_argument(\"--env\", type=str, default=\"drone_dataset\")\n",
    "\n",
    "# model options\n",
    "## Training Context Length K: (default: 20)\n",
    "parser.add_argument(\"--K\", type=int, default=40)\n",
    "## Embedding dimension: (default: 512)\n",
    "parser.add_argument(\"--embed_dim\", type=int, default=512)\n",
    "## Number of Layers: (default: 4)\n",
    "parser.add_argument(\"--n_layer\", type=int, default=8)\n",
    "## Number of Attention Heads: (default: 4)\n",
    "parser.add_argument(\"--n_head\", type=int, default=8)\n",
    "## Nonlinearity function: \n",
    "parser.add_argument(\"--activation_function\", type=str, default=\"relu\")\n",
    "## Dropout:\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "## Evaluating Context Length K: \n",
    "parser.add_argument(\"--eval_context_length\", type=int, default=10)\n",
    "## Positional embedding: absolute ordering\n",
    "parser.add_argument(\"--ordering\", type=int, default=1) # 0\n",
    "\n",
    "# shared evaluation options\n",
    "# g_eval: (default: 3600)\n",
    "parser.add_argument(\"--eval_rtg\", type=int, default=6000)\n",
    "parser.add_argument(\"--num_eval_episodes\", type=int, default=10)\n",
    "\n",
    "# shared training options\n",
    "parser.add_argument(\"--init_temperature\", type=float, default=0.1)\n",
    "## Batch Size: (default: 256)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "parser.add_argument(\"--learning_rate\", \"-lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--weight_decay\", \"-wd\", type=float, default=5e-4)\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=10000)\n",
    "\n",
    "# pretraining options\n",
    "parser.add_argument(\"--max_pretrain_iters\", type=int, default=1)\n",
    "parser.add_argument(\"--num_updates_per_pretrain_iter\", type=int, default=5000)\n",
    "\n",
    "# finetuning options\n",
    "parser.add_argument(\"--max_online_iters\", type=int, default=1500)\n",
    "parser.add_argument(\"--online_rtg\", type=int, default=7200)\n",
    "parser.add_argument(\"--num_online_rollouts\", type=int, default=1)\n",
    "parser.add_argument(\"--replay_size\", type=int, default=1000)\n",
    "parser.add_argument(\"--num_updates_per_online_iter\", type=int, default=300)\n",
    "parser.add_argument(\"--eval_interval\", type=int, default=10)\n",
    "\n",
    "# environment options\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "parser.add_argument(\"--log_to_tb\", \"-w\", type=bool, default=True)\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"./exp\")\n",
    "parser.add_argument(\"--exp_name\", type=str, default=\"default\")\n",
    "\n",
    "# general options\n",
    "parser.add_argument(\"--max_episode_len\", type=int, default=2000)\n",
    "\n",
    "# Add for fast Debbuging\n",
    "'''\n",
    "parser.add_argument(\"--K\", type=int, default=1)#####40)\n",
    "parser.add_argument(\"--embed_dim\", type=int, default=4)#####512)\n",
    "parser.add_argument(\"--n_layer\", type=int, default=2)#####8)\n",
    "parser.add_argument(\"--n_head\", type=int, default=2)#####8)\n",
    "parser.add_argument(\"--eval_context_length\", type=int, default=1)#####10) # d:5\n",
    "parser.add_argument(\"--eval_rtg\", type=int, default=600)#####6000)\n",
    "parser.add_argument(\"--num_eval_episodes\", type=int, default=2)#####10)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4)#####256)\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=10)#####10000)\n",
    "parser.add_argument(\"--num_updates_per_pretrain_iter\", type=int, default=500)#####5000)\n",
    "parser.add_argument(\"--max_online_iters\", type=int, default=500)#####1500)\n",
    "parser.add_argument(\"--online_rtg\", type=int, default=720)#####7200)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, variant):\n",
    "\n",
    "        self.state_dim, self.act_dim, self.action_range = self._get_env_spec(variant)\n",
    "        self.offline_trajs, self.state_mean, self.state_std = self._load_dataset(\n",
    "            variant[\"env\"]\n",
    "        )\n",
    "        # initialize by offline trajs\n",
    "        self.replay_buffer = ReplayBuffer(variant[\"replay_size\"], self.offline_trajs)\n",
    "\n",
    "        self.aug_trajs = []\n",
    "\n",
    "        self.device = variant.get(\"device\", \"cuda\")\n",
    "        self.target_entropy = -self.act_dim\n",
    "        self.model = DecisionTransformer(\n",
    "            state_dim=self.state_dim,\n",
    "            act_dim=self.act_dim,\n",
    "            action_range=self.action_range,\n",
    "            max_length=variant[\"K\"],\n",
    "            eval_context_length=variant[\"eval_context_length\"],\n",
    "            max_ep_len=variant[\"max_episode_len\"],\n",
    "            hidden_size=variant[\"embed_dim\"],\n",
    "            n_layer=variant[\"n_layer\"],\n",
    "            n_head=variant[\"n_head\"],\n",
    "            n_inner=4 * variant[\"embed_dim\"],\n",
    "            activation_function=variant[\"activation_function\"],\n",
    "            n_positions=1024,\n",
    "            resid_pdrop=variant[\"dropout\"],\n",
    "            attn_pdrop=variant[\"dropout\"],\n",
    "            stochastic_policy=True,\n",
    "            ordering=variant[\"ordering\"],\n",
    "            init_temperature=variant[\"init_temperature\"],\n",
    "            target_entropy=self.target_entropy,\n",
    "        ).to(device=self.device)\n",
    "\n",
    "        self.optimizer = Lamb(\n",
    "            self.model.parameters(),\n",
    "            lr=variant[\"learning_rate\"],\n",
    "            weight_decay=variant[\"weight_decay\"],\n",
    "            eps=1e-8,\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.optimizer, lambda steps: min((steps + 1) / variant[\"warmup_steps\"], 1)\n",
    "        )\n",
    "\n",
    "        self.log_temperature_optimizer = torch.optim.Adam(\n",
    "            [self.model.log_temperature],\n",
    "            lr=1e-4,\n",
    "            betas=[0.9, 0.999],\n",
    "        )\n",
    "\n",
    "        # track the training progress and\n",
    "        # training/evaluation/online performance in all the iterations\n",
    "        self.pretrain_iter = 0\n",
    "        self.online_iter = 0\n",
    "        self.total_transitions_sampled = 0\n",
    "        self.variant = variant\n",
    "        self.reward_scale = 1.0 if \"antmaze\" in variant[\"env\"] else 0.001\n",
    "        self.logger = Logger(variant)\n",
    "\n",
    "    def _get_env_spec(self, variant):\n",
    "        #####env = gym.make(variant[\"env\"])\n",
    "        env = make_pytorch_env(args)\n",
    "        #env.max_step = MAX_EPISODE_LEN\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.shape[0]\n",
    "        #action_range = [-0.999999, 0.999999]\n",
    "        \n",
    "        action_range = [\n",
    "            float(env.action_space.low.min()) + 1e-6,\n",
    "            float(env.action_space.high.max()) - 1e-6,\n",
    "        ]\n",
    "        \n",
    "        print(\"action_range: {}\".format(action_range))\n",
    "        env.close()\n",
    "        return state_dim, act_dim, action_range\n",
    "\n",
    "    def _save_model(self, path_prefix, is_pretrain_model=False):\n",
    "        to_save = {\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": self.scheduler.state_dict(),\n",
    "            \"pretrain_iter\": self.pretrain_iter,\n",
    "            \"online_iter\": self.online_iter,\n",
    "            \"args\": self.variant,\n",
    "            \"total_transitions_sampled\": self.total_transitions_sampled,\n",
    "            \"np\": np.random.get_state(),\n",
    "            \"python\": random.getstate(),\n",
    "            \"pytorch\": torch.get_rng_state(),\n",
    "            \"log_temperature_optimizer_state_dict\": self.log_temperature_optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        with open(f\"{path_prefix}/model.pt\", \"wb\") as f:\n",
    "            torch.save(to_save, f)\n",
    "        print(f\"\\nModel saved at {path_prefix}/model.pt\")\n",
    "\n",
    "        if is_pretrain_model:\n",
    "            with open(f\"{path_prefix}/pretrain_model.pt\", \"wb\") as f:\n",
    "                torch.save(to_save, f)\n",
    "            print(f\"Model saved at {path_prefix}/pretrain_model.pt\")\n",
    "\n",
    "    def _load_model(self, path_prefix):\n",
    "        if Path(f\"{path_prefix}/model.pt\").exists():\n",
    "            with open(f\"{path_prefix}/model.pt\", \"rb\") as f:\n",
    "                checkpoint = torch.load(f)\n",
    "            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "            self.log_temperature_optimizer.load_state_dict(\n",
    "                checkpoint[\"log_temperature_optimizer_state_dict\"]\n",
    "            )\n",
    "            self.pretrain_iter = checkpoint[\"pretrain_iter\"]\n",
    "            self.online_iter = checkpoint[\"online_iter\"]\n",
    "            self.total_transitions_sampled = checkpoint[\"total_transitions_sampled\"]\n",
    "            np.random.set_state(checkpoint[\"np\"])\n",
    "            random.setstate(checkpoint[\"python\"])\n",
    "            torch.set_rng_state(checkpoint[\"pytorch\"])\n",
    "            print(f\"Model loaded at {path_prefix}/model.pt\")\n",
    "\n",
    "    def _load_dataset(self, env_name):\n",
    "\n",
    "        dataset_path = f\"./data/{env_name}.pkl\"\n",
    "        with open(dataset_path, \"rb\") as f:\n",
    "            trajectories = pickle.load(f)\n",
    "\n",
    "        states, traj_lens, returns = [], [], []\n",
    "        for path in trajectories:\n",
    "            states.append(path[\"observations\"])\n",
    "            traj_lens.append(len(path[\"observations\"]))\n",
    "            returns.append(path[\"rewards\"].sum())\n",
    "        traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "        # used for input normalization\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        num_timesteps = sum(traj_lens)\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Starting new experiment: {env_name}\")\n",
    "        print(f\"{len(traj_lens)} trajectories, {num_timesteps} timesteps found\")\n",
    "        print(f\"Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}\")\n",
    "        print(f\"Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}\")\n",
    "        print(f\"Average length: {np.mean(traj_lens):.2f}, std: {np.std(traj_lens):.2f}\")\n",
    "        print(f\"Max length: {np.max(traj_lens):.2f}, min: {np.min(traj_lens):.2f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "        num_trajectories = 1\n",
    "        timesteps = traj_lens[sorted_inds[-1]]\n",
    "        ind = len(trajectories) - 2\n",
    "        while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] < num_timesteps:\n",
    "            timesteps += traj_lens[sorted_inds[ind]]\n",
    "            num_trajectories += 1\n",
    "            ind -= 1\n",
    "        sorted_inds = sorted_inds[-num_trajectories:]\n",
    "        trajectories = [trajectories[ii] for ii in sorted_inds]\n",
    "\n",
    "        return trajectories, state_mean, state_std\n",
    "\n",
    "    def _augment_trajectories(\n",
    "        self,\n",
    "        online_envs,\n",
    "        target_explore,\n",
    "        n,\n",
    "        randomized=False,\n",
    "    ):\n",
    "\n",
    "        max_ep_len = self.variant[\"max_episode_len\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generate init state\n",
    "            target_return = [target_explore * self.reward_scale] * online_envs.num_envs\n",
    "\n",
    "            returns, lengths, trajs = vec_evaluate_episode_rtg(\n",
    "                online_envs,\n",
    "                self.state_dim,\n",
    "                self.act_dim,\n",
    "                self.model,\n",
    "                max_ep_len=max_ep_len,\n",
    "                reward_scale=self.reward_scale,\n",
    "                target_return=target_return,\n",
    "                mode=\"normal\",\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                device=self.device,\n",
    "                use_mean=False,\n",
    "            )\n",
    "\n",
    "        self.replay_buffer.add_new_trajs(trajs)\n",
    "        self.aug_trajs += trajs\n",
    "        self.total_transitions_sampled += np.sum(lengths)\n",
    "\n",
    "        return {\n",
    "            \"aug_traj/return\": np.mean(returns),\n",
    "            \"aug_traj/length\": np.mean(lengths),\n",
    "        }\n",
    "\n",
    "    def pretrain(self, eval_envs, loss_fn):\n",
    "        print(\"\\n\\n\\n*** Pretrain ***\")\n",
    "        print(\"----------------\")\n",
    "        print(\"eval_envs: {}\".format(eval_envs))\n",
    "        print(\"loss_fn: {}\".format(loss_fn))\n",
    "        \n",
    "        eval_fns = [\n",
    "            create_vec_eval_episodes_fn(\n",
    "                vec_env=eval_envs,\n",
    "                eval_rtg=self.variant[\"eval_rtg\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                device=self.device,\n",
    "                use_mean=True,\n",
    "                reward_scale=self.reward_scale,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        trainer = SequenceTrainer(\n",
    "            model=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            log_temperature_optimizer=self.log_temperature_optimizer,\n",
    "            scheduler=self.scheduler,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        writer = (\n",
    "            SummaryWriter(self.logger.log_path) if self.variant[\"log_to_tb\"] else None\n",
    "        )\n",
    "        while self.pretrain_iter < self.variant[\"max_pretrain_iters\"]:\n",
    "            # in every iteration, prepare the data loader\n",
    "            dataloader = create_dataloader(\n",
    "                trajectories=self.offline_trajs,\n",
    "                num_iters=self.variant[\"num_updates_per_pretrain_iter\"],\n",
    "                batch_size=self.variant[\"batch_size\"],\n",
    "                max_len=self.variant[\"K\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                reward_scale=self.reward_scale,\n",
    "                action_range=self.action_range,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "\n",
    "            train_outputs = trainer.train_iteration(\n",
    "                loss_fn=loss_fn,\n",
    "                dataloader=dataloader,\n",
    "            )\n",
    "            eval_outputs, eval_reward = self.evaluate(eval_fns)\n",
    "            outputs = {\"time/total\": time.time() - self.start_time}\n",
    "            outputs.update(train_outputs)\n",
    "            outputs.update(eval_outputs)\n",
    "            self.logger.log_metrics(\n",
    "                outputs,\n",
    "                iter_num=self.pretrain_iter,\n",
    "                total_transitions_sampled=self.total_transitions_sampled,\n",
    "                writer=writer,\n",
    "            )\n",
    "\n",
    "            self._save_model(\n",
    "                path_prefix=self.logger.log_path,\n",
    "                is_pretrain_model=True,\n",
    "            )\n",
    "\n",
    "            self.pretrain_iter += 1\n",
    "\n",
    "    def evaluate(self, eval_fns):\n",
    "        eval_start = time.time()\n",
    "        self.model.eval()\n",
    "        outputs = {}\n",
    "        for eval_fn in eval_fns:\n",
    "            o = eval_fn(self.model)\n",
    "            outputs.update(o)\n",
    "        outputs[\"time/evaluation\"] = time.time() - eval_start\n",
    "\n",
    "        eval_reward = outputs[\"evaluation/return_mean_gm\"]\n",
    "        return outputs, eval_reward\n",
    "\n",
    "    def online_tuning(self, online_envs, eval_envs, loss_fn):\n",
    "\n",
    "        print(\"\\n\\n\\n*** Online Finetuning ***\")\n",
    "\n",
    "        trainer = SequenceTrainer(\n",
    "            model=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            log_temperature_optimizer=self.log_temperature_optimizer,\n",
    "            scheduler=self.scheduler,\n",
    "            device=self.device,\n",
    "        )\n",
    "        eval_fns = [\n",
    "            create_vec_eval_episodes_fn(\n",
    "                vec_env=eval_envs,\n",
    "                eval_rtg=self.variant[\"eval_rtg\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                device=self.device,\n",
    "                use_mean=True,\n",
    "                reward_scale=self.reward_scale,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "        ]\n",
    "        writer = (\n",
    "            SummaryWriter(self.logger.log_path) if self.variant[\"log_to_tb\"] else None\n",
    "        )\n",
    "        while self.online_iter < self.variant[\"max_online_iters\"]:\n",
    "\n",
    "            outputs = {}\n",
    "            augment_outputs = self._augment_trajectories(\n",
    "                online_envs,\n",
    "                self.variant[\"online_rtg\"],\n",
    "                n=self.variant[\"num_online_rollouts\"],\n",
    "            )\n",
    "            outputs.update(augment_outputs)\n",
    "\n",
    "            dataloader = create_dataloader(\n",
    "                trajectories=self.replay_buffer.trajectories,\n",
    "                num_iters=self.variant[\"num_updates_per_online_iter\"],\n",
    "                batch_size=self.variant[\"batch_size\"],\n",
    "                max_len=self.variant[\"K\"],\n",
    "                state_dim=self.state_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                state_mean=self.state_mean,\n",
    "                state_std=self.state_std,\n",
    "                reward_scale=self.reward_scale,\n",
    "                action_range=self.action_range,\n",
    "                max_episode_len = self.variant[\"max_episode_len\"],\n",
    "            )\n",
    "\n",
    "            # finetuning\n",
    "            is_last_iter = self.online_iter == self.variant[\"max_online_iters\"] - 1\n",
    "            if (self.online_iter + 1) % self.variant[\n",
    "                \"eval_interval\"\n",
    "            ] == 0 or is_last_iter:\n",
    "                evaluation = True\n",
    "            else:\n",
    "                evaluation = False\n",
    "\n",
    "            train_outputs = trainer.train_iteration(\n",
    "                loss_fn=loss_fn,\n",
    "                dataloader=dataloader,\n",
    "            )\n",
    "            outputs.update(train_outputs)\n",
    "\n",
    "            if evaluation:\n",
    "                eval_outputs, eval_reward = self.evaluate(eval_fns)\n",
    "                outputs.update(eval_outputs)\n",
    "\n",
    "            outputs[\"time/total\"] = time.time() - self.start_time\n",
    "\n",
    "            # log the metrics\n",
    "            self.logger.log_metrics(\n",
    "                outputs,\n",
    "                iter_num=self.pretrain_iter + self.online_iter,\n",
    "                total_transitions_sampled=self.total_transitions_sampled,\n",
    "                writer=writer,\n",
    "            )\n",
    "\n",
    "            self._save_model(\n",
    "                path_prefix=self.logger.log_path,\n",
    "                is_pretrain_model=False,\n",
    "            )\n",
    "\n",
    "            self.online_iter += 1\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        utils.set_seed_everywhere(args.seed)\n",
    "\n",
    "        import d4rl\n",
    "\n",
    "        def loss_fn(\n",
    "            a_hat_dist,     # action_preds\n",
    "            a,              # action_target\n",
    "            attention_mask, # padding_mask\n",
    "            entropy_reg,    # self.model.temperature().detach()\n",
    "        ):\n",
    "            # a_hat is a SquashedNormal Distribution\n",
    "            log_likelihood = a_hat_dist.log_likelihood(a)[attention_mask > 0].mean()\n",
    "            \n",
    "            entropy = a_hat_dist.entropy().mean()\n",
    "            loss = -(log_likelihood + entropy_reg * entropy)\n",
    "            \n",
    "            '''\n",
    "            print(\"a_hat_dist : {}\".format(a_hat_dist))\n",
    "            print(\"a : {}\".format(a))\n",
    "            torch.save(a,\"a.pt\")\n",
    "            print(\"a_hat_dist.log_likelihood(a) : {}\".format(a_hat_dist.log_likelihood(a)))\n",
    "            #print(\"attention_mask : {}\".format(attention_mask))\n",
    "            print(\"log_likelihood: {}\".format(log_likelihood))\n",
    "            print(\"loss inside jupyter: {} of type: {}\".format(loss,type(loss)))\n",
    "            '''\n",
    "            \n",
    "            return (\n",
    "                loss,\n",
    "                -log_likelihood,\n",
    "                entropy,\n",
    "            )\n",
    "\n",
    "        def get_env_builder(seed, env_name, target_goal=None):\n",
    "            def make_env_fn():\n",
    "                import d4rl\n",
    "\n",
    "                #####env = gym.make(env_name)\n",
    "                env = make_pytorch_env(args)\n",
    "                #env.max_step = MAX_EPISODE_LEN\n",
    "                env.seed(seed)\n",
    "                '''\n",
    "                if hasattr(env.env, \"wrapped_env\"):\n",
    "                    env.env.wrapped_env.seed(seed)\n",
    "                elif hasattr(env.env, \"seed\"):\n",
    "                    env.env.seed(seed)\n",
    "                else:\n",
    "                    pass\n",
    "                '''\n",
    "                '''\n",
    "                env.action_space.seed(seed)\n",
    "                env.observation_space.seed(seed)\n",
    "                '''\n",
    "\n",
    "                if target_goal:\n",
    "                    env.set_target_goal(target_goal)\n",
    "                    print(f\"Set the target goal to be {env.target_goal}\")\n",
    "                return env\n",
    "\n",
    "            return make_env_fn\n",
    "\n",
    "        print(\"\\n\\nMaking Eval Env.....\")\n",
    "        env_name = self.variant[\"env\"]\n",
    "        if \"antmaze\" in env_name:\n",
    "            env = gym.make(env_name)\n",
    "            target_goal = env.target_goal\n",
    "            env.close()\n",
    "            print(f\"Generated the fixed target goal: {target_goal}\")\n",
    "        else:\n",
    "            target_goal = None\n",
    "        eval_envs = SubprocVecEnv(\n",
    "            [\n",
    "                get_env_builder(i, env_name=env_name, target_goal=target_goal)\n",
    "                for i in range(self.variant[\"num_eval_episodes\"])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        if self.variant[\"max_pretrain_iters\"]:\n",
    "            self.pretrain(eval_envs, loss_fn)\n",
    "        \n",
    "        if self.variant[\"max_online_iters\"]:\n",
    "            print(\"\\n\\nMaking Online Env.....\")\n",
    "            online_envs = SubprocVecEnv(\n",
    "                [\n",
    "                    get_env_builder(i + 100, env_name=env_name, target_goal=target_goal)\n",
    "                    for i in range(self.variant[\"num_online_rollouts\"])\n",
    "                ]\n",
    "            )\n",
    "            self.online_tuning(online_envs, eval_envs, loss_fn)\n",
    "            online_envs.close()\n",
    "\n",
    "        eval_envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428b67f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.set_seed_everywhere(args.seed)\n",
    "experiment = Experiment(vars(args))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ae700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_env(env):\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    action_range = [\n",
    "        float(env.action_space.low.min()) + 1e-6,\n",
    "        float(env.action_space.high.max()) - 1e-6]\n",
    "        \n",
    "    print(\"state_dim: {}\".format(state_dim))\n",
    "    print(\"act_dim: {}\".format(act_dim))\n",
    "    print(\"action_range: {}\".format(action_range))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a72dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env = make_pytorch_env(args)\n",
    "their_env = gym.make('antmaze-large-diverse-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_env(my_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_env(their_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ffe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env.reset()\n",
    "my_env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234518ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70610f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "their_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "their_env.reset()\n",
    "their_env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac790187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment.variant\n",
    "#experiment.model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1b18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ec8a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2bbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c547220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e41b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.log(1e-310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db722b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds = torch.load('action_preds.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"a.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e15e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.log_likelihood(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b65b461",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sefude = action_preds.log_likelihood(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nan_to_num(sefude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f47b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc3b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3641331",
   "metadata": {},
   "outputs": [],
   "source": [
    "math.log(-0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07872499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.entropy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7aefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.log_likelihood(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds.perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "state_dim = 4\n",
    "hidden_size = 512\n",
    "\n",
    "embed_state = torch.nn.Linear(state_dim, hidden_size).to('cuda')\n",
    "embed_state_2 = torch.load('embed_state.pt').to('cuda')\n",
    "states = torch.load('states.pt').to('cuda')\n",
    "state_embeddings = embed_state(states)\n",
    "state_embeddings_2 = torch.load('state_embeddings.pt').to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92878d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e8be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"state_embeddings {}\".format(state_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26ca72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"state_embeddings 2 {}\".format(state_embeddings_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state_2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_state_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790f259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_state(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da41c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_state_2(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2bf17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737fe07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoppppppppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b433b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c390a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando as rewards pra ver se resolve o problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d99eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/drone_dataset.pkl', 'rb') as f:\n",
    "    my_data = pickle.load(f)\n",
    "    \n",
    "with open('data/antmaze-large-diverse-v2.pkl', 'rb') as f:\n",
    "    their_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in my_data:\n",
    "    rewards = data['actions']\n",
    "    print(\"max: {}\".format(np.max(rewards)))\n",
    "    print(\"min: {}\".format(np.min(rewards)))\n",
    "    print(\"mean: {}\".format(np.mean(rewards)))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(my_data[0]['observations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b79cc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.shape(their_data[0]['observations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308b8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7a8eb4bc",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/drone_dataset.pkl', 'rb') as f:\n",
    "    my_data = pickle.load(f)\n",
    "    \n",
    "\n",
    "for data in my_data:\n",
    "    \n",
    "    #data['rewards']   = np.float32(data['rewards'].flatten())\n",
    "    #data['terminals'] = np.float32(data['terminals'].flatten())\n",
    "    data['actions'] = np.float32(np.minimum(np.maximum(data['actions'], -1), 1))\n",
    "    #data['observations'] = np.float32(data['observations'])\n",
    "    #data['next_observations'] = np.float32(data['next_observations'])\n",
    "    \n",
    "with open('data/drone_dataset.pkl', 'wb') as handle:\n",
    "    pickle.dump(my_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96eb78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(v - v.min()) / (v.max() - v.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c58b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
