{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda414bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PyQt5.QtCore import QtMsgType, QMessageLogContext, qInstallMessageHandler\n",
    "\n",
    "def messageHandler(type_, context, message):\n",
    "    #if \"QObject::moveToThread: Current thread is not the object's thread\" not in message:\n",
    "    if \"moveToThread\" not in message:\n",
    "        # Se a mensagem não contém o aviso que deseja desativar, imprima normalmente.\n",
    "        sys.__stdout__.write(message)\n",
    "    else:\n",
    "        # Se a mensagem contém o aviso que deseja desativar, ignore-a.\n",
    "        pass\n",
    "\n",
    "# Substitua o manipulador de mensagens padrão do PyQt ou PySide pelo nosso manipulador personalizado.\n",
    "qInstallMessageHandler(messageHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbc654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:39.263609Z",
     "start_time": "2023-03-21T11:17:38.565541Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def calculate_tensors():\n",
    "    num_tensors=0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                #print(type(obj), obj.size())\n",
    "                num_tensors+=1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    print(\"num_tensors: {}\".format(num_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a2fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:39.679406Z",
     "start_time": "2023-03-21T11:17:39.264416Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from env import make_pytorch_env\n",
    "from decision_transformer.models.decision_transformer import DecisionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d6c7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:39.681252Z",
     "start_time": "2023-03-21T11:17:39.680058Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    # vars to class\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f261c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:41.196313Z",
     "start_time": "2023-03-21T11:17:39.681977Z"
    }
   },
   "outputs": [],
   "source": [
    "#loaded_model = torch.load(\"./exp/2023.03.16/170104-default/model.pt\")\n",
    "#loaded_pretrain_model = torch.load(\"./exp/2023.03.16/170104-default/pretrain_model.pt\")\n",
    "\n",
    "#saved_model_name = '2023.03.20/222713'\n",
    "#saved_model_name = '2023.03.22/120246'\n",
    "saved_model_name = '2023.04.11/192215'\n",
    "loaded_model = torch.load(\"./exp/{}-default/model.pt\".format(saved_model_name))\n",
    "\n",
    "\n",
    "variant = loaded_model['args']\n",
    "args = MyClass(**variant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9eccc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:41.199484Z",
     "start_time": "2023-03-21T11:17:41.198062Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e721553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:41.202611Z",
     "start_time": "2023-03-21T11:17:41.201268Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_env_spec(variant):\n",
    "        #####env = gym.make(variant[\"env\"])\n",
    "        env = make_pytorch_env(args)\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.shape[0]\n",
    "        \n",
    "        action_range = [\n",
    "            float(env.action_space.low.min()) + 1e-6,\n",
    "            float(env.action_space.high.max()) - 1e-6,\n",
    "        ]\n",
    "        \n",
    "        env.close()\n",
    "        return state_dim, act_dim, action_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47ec37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:41.394969Z",
     "start_time": "2023-03-21T11:17:41.204701Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dim, act_dim, action_range = _get_env_spec(vars(args))\n",
    "target_entropy = -act_dim\n",
    "\n",
    "#MAX_EPISODE_LEN = 2000 # 4000 # 4000\n",
    "\n",
    "model = DecisionTransformer(\n",
    "            state_dim=state_dim,\n",
    "            act_dim=act_dim,\n",
    "            action_range=action_range,\n",
    "            max_length=variant[\"K\"],\n",
    "            eval_context_length=variant[\"eval_context_length\"],\n",
    "            max_ep_len=variant['max_episode_len'],\n",
    "            hidden_size=variant[\"embed_dim\"],\n",
    "            n_layer=variant[\"n_layer\"],\n",
    "            n_head=variant[\"n_head\"],\n",
    "            n_inner=4 * variant[\"embed_dim\"],\n",
    "            activation_function=variant[\"activation_function\"],\n",
    "            n_positions=1024,\n",
    "            n_ctx=3*variant[\"K\"],\n",
    "            resid_pdrop=variant[\"dropout\"],\n",
    "            attn_pdrop=variant[\"dropout\"],\n",
    "            stochastic_policy=True,\n",
    "            ordering=variant[\"ordering\"],\n",
    "            init_temperature=variant[\"init_temperature\"],\n",
    "            target_entropy=target_entropy,\n",
    "        ).to(device=args.device)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3460d0",
   "metadata": {},
   "source": [
    "## Rascunhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b423441e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:41.395107Z",
     "start_time": "2023-03-21T11:17:41.387963Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2666f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:41.436252Z",
     "start_time": "2023-03-21T11:17:41.392457Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: save state_mean, state_std\n",
    "\n",
    "def _load_dataset(env_name):\n",
    "\n",
    "        dataset_path = f\"./data/{env_name}.pkl\"\n",
    "        with open(dataset_path, \"rb\") as f:\n",
    "            trajectories = pickle.load(f)\n",
    "\n",
    "        states, traj_lens, returns = [], [], []\n",
    "        for path in trajectories:\n",
    "            states.append(path[\"observations\"])\n",
    "            traj_lens.append(len(path[\"observations\"]))\n",
    "            returns.append(path[\"rewards\"].sum())\n",
    "        traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "        # used for input normalization\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        num_timesteps = sum(traj_lens)\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Starting new experiment: {env_name}\")\n",
    "        print(f\"{len(traj_lens)} trajectories, {num_timesteps} timesteps found\")\n",
    "        print(f\"Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}\")\n",
    "        print(f\"Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}\")\n",
    "        print(f\"Average length: {np.mean(traj_lens):.2f}, std: {np.std(traj_lens):.2f}\")\n",
    "        print(f\"Max length: {np.max(traj_lens):.2f}, min: {np.min(traj_lens):.2f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "        num_trajectories = 1\n",
    "        timesteps = traj_lens[sorted_inds[-1]]\n",
    "        ind = len(trajectories) - 2\n",
    "        while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] < num_timesteps:\n",
    "            timesteps += traj_lens[sorted_inds[ind]]\n",
    "            num_trajectories += 1\n",
    "            ind -= 1\n",
    "        sorted_inds = sorted_inds[-num_trajectories:]\n",
    "        trajectories = [trajectories[ii] for ii in sorted_inds]\n",
    "\n",
    "        return trajectories, state_mean, state_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5dee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:41.436465Z",
     "start_time": "2023-03-21T11:17:41.433946Z"
    }
   },
   "outputs": [],
   "source": [
    "device=args.device\n",
    "torch.no_grad()\n",
    "\n",
    "#Load the weights on the model\n",
    "model.load_state_dict(loaded_model['model_state_dict'])\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "# Convert model to GPU\n",
    "model.to(device=args.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f948859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:41.921707Z",
     "start_time": "2023-03-21T11:17:41.434047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Nao gostei disso pq tem a ver com o Dataset\n",
    "offline_trajs, state_mean, state_std = _load_dataset(args.env)\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ece00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T11:17:41.925792Z",
     "start_time": "2023-03-21T11:17:41.923559Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_env = make_pytorch_env(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa9ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ep_len = 2000\n",
    "env = vec_env\n",
    "env.max_step = max_ep_len\n",
    "\n",
    "env.reset()\n",
    "env.close()\n",
    "\n",
    "terminal = False\n",
    "\n",
    "while not terminal:\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action =  [1,1,1] # mode.get_predictions blah\n",
    "    \n",
    "    #action = np.array([3.0,3.0,3.0])\n",
    "    print(env.current_step)\n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    #env.render()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5fd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe04443a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 1\n",
    "reward_scale = 1.0 if \"antmaze\" in variant[\"env\"] else 0.001\n",
    "\n",
    "#max_ep_len = MAX_EPISODE_LEN\n",
    "#max_ep_len = 4000\n",
    "#max_ep_len = 2000 # WARNING! JUST FOR DEBBUGING\n",
    "#vec_env.max_step = max_ep_len\n",
    "num_test_episodes = 20\n",
    "use_mean = True # False # True\n",
    "mode = 'normal' # delayed\n",
    "vec_env.cv2_show_render = True  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c8ba0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(num_test_episodes):\n",
    "\n",
    "    ###\n",
    "    state = vec_env.reset()\n",
    "    unfinished = np.ones(num_envs).astype(bool)\n",
    "    # Not sure:\n",
    "    target_return = [variant['eval_rtg'] * reward_scale] * num_envs\n",
    "\n",
    "    ep_return = target_return\n",
    "\n",
    "    target_return = torch.tensor(ep_return, device=device, dtype=torch.float32).reshape(\n",
    "        num_envs, -1, 1\n",
    "    )\n",
    "    timesteps = torch.tensor([0] * num_envs, device=device, dtype=torch.long).reshape(\n",
    "        num_envs, -1\n",
    "    )\n",
    "\n",
    "\n",
    "    states = (\n",
    "        torch.from_numpy(state)\n",
    "        .reshape(num_envs, state_dim)\n",
    "        .to(device=device, dtype=torch.float32)\n",
    "    ).reshape(num_envs, -1, state_dim)\n",
    "\n",
    "    actions = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "    rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "    ep_return = target_return\n",
    "    target_return = torch.tensor(ep_return, device=device, dtype=torch.float32).reshape(\n",
    "        num_envs, -1, 1\n",
    "    )\n",
    "    timesteps = torch.tensor([0] * num_envs, device=device, dtype=torch.long).reshape(\n",
    "        num_envs, -1\n",
    "    )\n",
    "\n",
    "    # episode_return, episode_length = 0.0, 0\n",
    "    episode_return = np.zeros((num_envs, 1)).astype(float)\n",
    "    episode_length = np.full(num_envs, np.inf)\n",
    "\n",
    "    # TODO: read the paper and figure it out if reward state is necessary\n",
    "\n",
    "    ###\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for t in range(max_ep_len):\n",
    "            # add padding\n",
    "            actions = torch.cat(\n",
    "                [\n",
    "                    actions,\n",
    "                    torch.zeros((num_envs, act_dim), device=device).reshape(\n",
    "                        num_envs, -1, act_dim\n",
    "                    ),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            rewards = torch.cat(\n",
    "                [\n",
    "                    rewards,\n",
    "                    torch.zeros((num_envs, 1), device=device).reshape(num_envs, -1, 1),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            state_pred, action_dist, reward_pred = model.get_predictions(\n",
    "                (states.to(dtype=torch.float32) - state_mean) / state_std,\n",
    "                actions.to(dtype=torch.float32),\n",
    "                rewards.to(dtype=torch.float32),\n",
    "                target_return.to(dtype=torch.float32),\n",
    "                timesteps.to(dtype=torch.long),\n",
    "                num_envs=num_envs,\n",
    "            )\n",
    "            state_pred = state_pred.detach().cpu().numpy().reshape(num_envs, -1)\n",
    "            reward_pred = reward_pred.detach().cpu().numpy().reshape(num_envs)\n",
    "\n",
    "            # the return action is a SquashNormal distribution\n",
    "            action = action_dist.sample().reshape(num_envs, -1, act_dim)[:, -1]\n",
    "            if use_mean:\n",
    "                action = action_dist.mean.reshape(num_envs, -1, act_dim)[:, -1]\n",
    "            action = action.clamp(*model.action_range)\n",
    "\n",
    "            # TODO: nao entendo pq esta gerando um [] a mais e se isso atrapalhou no training\n",
    "            #print(\"action: {}\".format(action[0]))\n",
    "            state, reward, done, _ = vec_env.step(action.detach().cpu().numpy()[0])\n",
    "            vec_env.render()\n",
    "                \n",
    "            #state, reward, done, _ = vec_env.step(action.detach().cpu().numpy())\n",
    "\n",
    "            # eval_env.step() will execute the action for all the sub-envs, for those where\n",
    "            # the episodes have terminated, the envs will be reset. Hence we use\n",
    "            # \"unfinished\" to track whether the first episode we roll out for each sub-env is\n",
    "            # finished. In contrast, \"done\" only relates to the current episode\n",
    "            # TODO: nao sei pq, mas o unfinished precisa por [0]\n",
    "            episode_return[unfinished] += reward[unfinished[0]].reshape(-1, 1)\n",
    "            #episode_return[unfinished] += reward[unfinished[0]].reshape(-1, 1)\n",
    "\n",
    "            actions[:, -1] = action\n",
    "            state = (\n",
    "                torch.from_numpy(state).to(device=device).reshape(num_envs, -1, state_dim)\n",
    "            )\n",
    "            states = torch.cat([states, state], dim=1)\n",
    "            #print(\"states: {}\".format(states))\n",
    "            # TODO: n sei pq, mas tive que por np.array em reward (na vdd sei, apenas 1 evaluate..)\n",
    "            reward = torch.from_numpy(np.array(reward)).to(device=device).reshape(num_envs, 1)\n",
    "            #reward = torch.from_numpy(reward).to(device=device).reshape(num_envs, 1)\n",
    "            rewards[:, -1] = reward\n",
    "\n",
    "            if mode != \"delayed\":\n",
    "                pred_return = target_return[:, -1] - (reward * reward_scale)\n",
    "            else:\n",
    "                pred_return = target_return[:, -1]\n",
    "            target_return = torch.cat(\n",
    "                [target_return, pred_return.reshape(num_envs, -1, 1)], dim=1\n",
    "            )\n",
    "\n",
    "            timesteps = torch.cat(\n",
    "                [\n",
    "                    timesteps,\n",
    "                    torch.ones((num_envs, 1), device=device, dtype=torch.long).reshape(\n",
    "                        num_envs, 1\n",
    "                    )\n",
    "                    * (t + 1),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            \n",
    "            # TODO: I comment this below, but I think it is important\n",
    "            '''\n",
    "            if t == max_ep_len - 1:\n",
    "                done = np.ones(done.shape).astype(bool)\n",
    "            if np.any(done):\n",
    "                ind = np.where(done)[0]\n",
    "                unfinished[ind] = False\n",
    "                episode_length[ind] = np.minimum(episode_length[ind], t + 1)\n",
    "\n",
    "            if not np.any(unfinished):\n",
    "                break\n",
    "            '''\n",
    "        print(\"Episode Return: {}\".format(episode_return[0][0]))\n",
    "        #calculate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2df21f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "gc.get_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab34f0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "calculate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcc9e8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b920ac3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984ae25",
   "metadata": {},
   "source": [
    "## Fim dos Rascunhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5bd6af",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73875ed0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0cd0c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8311f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.close()\n",
    "\n",
    "terminal = False\n",
    "\n",
    "while not terminal:\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action = model(env.state) # mode.get_predictions blah\n",
    "    \n",
    "    #action = np.array([3.0,3.0,3.0])\n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec5374",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
