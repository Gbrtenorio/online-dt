{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9edbc654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def calculate_tensors():\n",
    "    num_tensors=0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                #print(type(obj), obj.size())\n",
    "                num_tensors+=1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    print(\"num_tensors: {}\".format(num_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8a2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from env import make_pytorch_env\n",
    "from decision_transformer.models.decision_transformer import DecisionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c1d6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    # vars to class\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08f261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(\"./exp/2023.03.16/170104-default/model.pt\")\n",
    "loaded_pretrain_model = torch.load(\"./exp/2023.03.16/170104-default/pretrain_model.pt\")\n",
    "\n",
    "variant = loaded_model['args']\n",
    "args = MyClass(**variant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c9eccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'pretrain_iter', 'online_iter', 'args', 'total_transitions_sampled', 'np', 'python', 'pytorch', 'log_temperature_optimizer_state_dict'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e721553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_env_spec(variant):\n",
    "        #####env = gym.make(variant[\"env\"])\n",
    "        env = make_pytorch_env(args)\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.shape[0]\n",
    "        \n",
    "        action_range = [\n",
    "            float(env.action_space.low.min()) + 1e-6,\n",
    "            float(env.action_space.high.max()) - 1e-6,\n",
    "        ]\n",
    "        \n",
    "        env.close()\n",
    "        return state_dim, act_dim, action_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee47ec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/odt/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "state_dim, act_dim, action_range = _get_env_spec(vars(args))\n",
    "target_entropy = -act_dim\n",
    "\n",
    "MAX_EPISODE_LEN = 4000\n",
    "\n",
    "model = DecisionTransformer(\n",
    "            state_dim=state_dim,\n",
    "            act_dim=act_dim,\n",
    "            action_range=action_range,\n",
    "            max_length=variant[\"K\"],\n",
    "            eval_context_length=variant[\"eval_context_length\"],\n",
    "            max_ep_len=MAX_EPISODE_LEN,\n",
    "            hidden_size=variant[\"embed_dim\"],\n",
    "            n_layer=variant[\"n_layer\"],\n",
    "            n_head=variant[\"n_head\"],\n",
    "            n_inner=4 * variant[\"embed_dim\"],\n",
    "            activation_function=variant[\"activation_function\"],\n",
    "            n_positions=1024,\n",
    "            resid_pdrop=variant[\"dropout\"],\n",
    "            attn_pdrop=variant[\"dropout\"],\n",
    "            stochastic_policy=True,\n",
    "            ordering=variant[\"ordering\"],\n",
    "            init_temperature=variant[\"init_temperature\"],\n",
    "            target_entropy=target_entropy,\n",
    "        ).to(device=args.device)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3460d0",
   "metadata": {},
   "source": [
    "## Rascunhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b423441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f2666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: save state_mean, state_std\n",
    "\n",
    "def _load_dataset(env_name):\n",
    "\n",
    "        dataset_path = f\"./data/{env_name}.pkl\"\n",
    "        with open(dataset_path, \"rb\") as f:\n",
    "            trajectories = pickle.load(f)\n",
    "\n",
    "        states, traj_lens, returns = [], [], []\n",
    "        for path in trajectories:\n",
    "            states.append(path[\"observations\"])\n",
    "            traj_lens.append(len(path[\"observations\"]))\n",
    "            returns.append(path[\"rewards\"].sum())\n",
    "        traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "        # used for input normalization\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        num_timesteps = sum(traj_lens)\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Starting new experiment: {env_name}\")\n",
    "        print(f\"{len(traj_lens)} trajectories, {num_timesteps} timesteps found\")\n",
    "        print(f\"Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}\")\n",
    "        print(f\"Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}\")\n",
    "        print(f\"Average length: {np.mean(traj_lens):.2f}, std: {np.std(traj_lens):.2f}\")\n",
    "        print(f\"Max length: {np.max(traj_lens):.2f}, min: {np.min(traj_lens):.2f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "        num_trajectories = 1\n",
    "        timesteps = traj_lens[sorted_inds[-1]]\n",
    "        ind = len(trajectories) - 2\n",
    "        while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] < num_timesteps:\n",
    "            timesteps += traj_lens[sorted_inds[ind]]\n",
    "            num_trajectories += 1\n",
    "            ind -= 1\n",
    "        sorted_inds = sorted_inds[-num_trajectories:]\n",
    "        trajectories = [trajectories[ii] for ii in sorted_inds]\n",
    "\n",
    "        return trajectories, state_mean, state_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb5dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=args.device\n",
    "torch.no_grad()\n",
    "\n",
    "#Load the weights on the model\n",
    "model.load_state_dict(loaded_model['model_state_dict'])\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "# Convert model to GPU\n",
    "model.to(device=args.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f948859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting new experiment: drone_dataset\n",
      "1544 trajectories, 3497627 timesteps found\n",
      "Average return: -29.00, std: 2362.99\n",
      "Max return: 3362.87, min: -5541.95\n",
      "Average length: 2265.30, std: 1012.84\n",
      "Max length: 4001.00, min: 919.00\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Nao gostei disso pq tem a ver com o Dataset\n",
    "offline_trajs, state_mean, state_std = _load_dataset(args.env)\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0ece00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = make_pytorch_env(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee0eb562",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 1\n",
    "reward_scale = 1.0 if \"antmaze\" in variant[\"env\"] else 0.001\n",
    "\n",
    "max_ep_len = 4000\n",
    "use_mean = True # False # True\n",
    "state = vec_env.reset()\n",
    "unfinished = np.ones(num_envs).astype(bool)\n",
    "mode = 'normal' # delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2044a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure:\n",
    "target_return = [variant['eval_rtg'] * reward_scale] * num_envs\n",
    "\n",
    "ep_return = target_return\n",
    "\n",
    "target_return = torch.tensor(ep_return, device=device, dtype=torch.float32).reshape(\n",
    "    num_envs, -1, 1\n",
    ")\n",
    "timesteps = torch.tensor([0] * num_envs, device=device, dtype=torch.long).reshape(\n",
    "    num_envs, -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94f3309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15998/1257073245.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_return = torch.tensor(ep_return, device=device, dtype=torch.float32).reshape(\n"
     ]
    }
   ],
   "source": [
    "states = (\n",
    "    torch.from_numpy(state)\n",
    "    .reshape(num_envs, state_dim)\n",
    "    .to(device=device, dtype=torch.float32)\n",
    ").reshape(num_envs, -1, state_dim)\n",
    "\n",
    "actions = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "ep_return = target_return\n",
    "target_return = torch.tensor(ep_return, device=device, dtype=torch.float32).reshape(\n",
    "    num_envs, -1, 1\n",
    ")\n",
    "timesteps = torch.tensor([0] * num_envs, device=device, dtype=torch.long).reshape(\n",
    "    num_envs, -1\n",
    ")\n",
    "\n",
    "# episode_return, episode_length = 0.0, 0\n",
    "episode_return = np.zeros((num_envs, 1)).astype(float)\n",
    "episode_length = np.full(num_envs, np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e4869c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_ep_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42e9928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: read the paper and figure it out if reward state is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f32b1843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "385c8ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/odt/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/home/gabriel/miniconda3/envs/odt/lib/python3.8/site-packages/scipy/_lib/_util.py:202: DeprecationWarning: Module scipy.linalg.blas.clapack is deprecated, use scipy.linalg.lapack instead\n",
      "  warnings.warn(\"Module %s is deprecated, use %s instead\"\n",
      "/home/gabriel/miniconda3/envs/odt/lib/python3.8/site-packages/scipy/_lib/_util.py:202: DeprecationWarning: Module scipy.linalg.blas.flapack is deprecated, use scipy.linalg.lapack instead\n",
      "  warnings.warn(\"Module %s is deprecated, use %s instead\"\n",
      "/home/gabriel/miniconda3/envs/odt/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/home/gabriel/miniconda3/envs/odt/lib/python3.8/site-packages/scipy/_lib/_util.py:202: DeprecationWarning: Module scipy.linalg.blas.clapack is deprecated, use scipy.linalg.lapack instead\n",
      "  warnings.warn(\"Module %s is deprecated, use %s instead\"\n",
      "/home/gabriel/miniconda3/envs/odt/lib/python3.8/site-packages/scipy/_lib/_util.py:202: DeprecationWarning: Module scipy.linalg.blas.flapack is deprecated, use scipy.linalg.lapack instead\n",
      "  warnings.warn(\"Module %s is deprecated, use %s instead\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tensors: 564\n",
      "num_tensors: 624\n",
      "num_tensors: 684\n",
      "num_tensors: 744\n",
      "num_tensors: 804\n",
      "num_tensors: 864\n",
      "num_tensors: 924\n",
      "num_tensors: 984\n",
      "num_tensors: 1044\n",
      "num_tensors: 1104\n",
      "num_tensors: 1164\n",
      "num_tensors: 1224\n",
      "num_tensors: 1284\n",
      "num_tensors: 1344\n",
      "num_tensors: 1404\n",
      "num_tensors: 1464\n",
      "num_tensors: 1524\n",
      "num_tensors: 1584\n",
      "num_tensors: 1644\n",
      "num_tensors: 1704\n",
      "num_tensors: 1764\n",
      "num_tensors: 1824\n",
      "num_tensors: 1884\n",
      "num_tensors: 1944\n",
      "num_tensors: 2004\n",
      "num_tensors: 2064\n",
      "num_tensors: 2124\n",
      "num_tensors: 2184\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m      3\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m      4\u001b[0m     [\n\u001b[1;32m      5\u001b[0m         actions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m     13\u001b[0m     [\n\u001b[1;32m     14\u001b[0m         rewards,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m state_pred, action_dist, reward_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_mean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_return\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m state_pred \u001b[38;5;241m=\u001b[39m state_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(num_envs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m reward_pred \u001b[38;5;241m=\u001b[39m reward_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(num_envs)\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/decision_transformer/models/decision_transformer.py:415\u001b[0m, in \u001b[0;36mDecisionTransformer.get_predictions\u001b[0;34m(self, states, actions, rewards, returns_to_go, timesteps, num_envs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    413\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m state_preds, action_preds, return_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturns_to_go\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mordering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic_policy:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state_preds[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], action_preds, return_preds[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/decision_transformer/models/decision_transformer.py:288\u001b[0m, in \u001b[0;36mDecisionTransformer.forward\u001b[0;34m(self, states, actions, rewards, returns_to_go, timesteps, ordering, padding_mask)\u001b[0m\n\u001b[1;32m    281\u001b[0m stacked_padding_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    282\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack((padding_mask, padding_mask, padding_mask), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m seq_length)\n\u001b[1;32m    285\u001b[0m )\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# we feed in the input embeddings (not word indices as in NLP) to the model\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacked_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacked_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m x \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m#print(\"x: {}\".format(x))\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m#print(\"stacked_inputs: {}\".format(stacked_inputs))\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m#print(\"stacked_padding_mask: {}\".format(stacked_padding_mask))\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# reshape x so that the second dimension corresponds to the original\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# returns (0), states (1), or actions (2); i.e. x[:,1,t] is the token for s_t\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/decision_transformer/models/trajectory_gpt2.py:733\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    723\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    724\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    725\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    730\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    731\u001b[0m     )\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 733\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m hidden_states, present \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/decision_transformer/models/trajectory_gpt2.py:303\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    294\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    302\u001b[0m ):\n\u001b[0;32m--> 303\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PhD_Researches/RL_Map/drone-gym-offline_online_rl_v2/online-dt/decision_transformer/models/trajectory_gpt2.py:221\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    219\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     query, key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    223\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(query)\n\u001b[1;32m    224\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(key, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/site-packages/transformers/modeling_utils.py:1222\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m   1221\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m-> 1222\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39msize_out)\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/site-packages/torch/fx/traceback.py:57\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m current_stack\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/traceback.py:197\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m format_list(\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/traceback.py:347\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    345\u001b[0m result \u001b[38;5;241m=\u001b[39m klass()\n\u001b[1;32m    346\u001b[0m fnames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[1;32m    348\u001b[0m     co \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mf_code\n\u001b[1;32m    349\u001b[0m     filename \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mco_filename\n",
      "File \u001b[0;32m~/miniconda3/envs/odt/lib/python3.8/traceback.py:301\u001b[0m, in \u001b[0;36mwalk_stack\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    299\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f, \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_lineno\u001b[49m\n\u001b[1;32m    302\u001b[0m     f \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mf_back\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(max_ep_len):\n",
    "    # add padding\n",
    "    actions = torch.cat(\n",
    "        [\n",
    "            actions,\n",
    "            torch.zeros((num_envs, act_dim), device=device).reshape(\n",
    "                num_envs, -1, act_dim\n",
    "            ),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    rewards = torch.cat(\n",
    "        [\n",
    "            rewards,\n",
    "            torch.zeros((num_envs, 1), device=device).reshape(num_envs, -1, 1),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    state_pred, action_dist, reward_pred = model.get_predictions(\n",
    "        (states.to(dtype=torch.float32) - state_mean) / state_std,\n",
    "        actions.to(dtype=torch.float32),\n",
    "        rewards.to(dtype=torch.float32),\n",
    "        target_return.to(dtype=torch.float32),\n",
    "        timesteps.to(dtype=torch.long),\n",
    "        num_envs=num_envs,\n",
    "    )\n",
    "    state_pred = state_pred.detach().cpu().numpy().reshape(num_envs, -1)\n",
    "    reward_pred = reward_pred.detach().cpu().numpy().reshape(num_envs)\n",
    "\n",
    "    # the return action is a SquashNormal distribution\n",
    "    action = action_dist.sample().reshape(num_envs, -1, act_dim)[:, -1]\n",
    "    if use_mean:\n",
    "        action = action_dist.mean.reshape(num_envs, -1, act_dim)[:, -1]\n",
    "    action = action.clamp(*model.action_range)\n",
    "\n",
    "    # TODO: nao entendo pq esta gerando um [] a mais e se isso atrapalhou no training\n",
    "    #print(\"action: {}\".format(action[0]))\n",
    "    state, reward, done, _ = vec_env.step(action.detach().cpu().numpy()[0])\n",
    "    #vec_env.render()\n",
    "    #state, reward, done, _ = vec_env.step(action.detach().cpu().numpy())\n",
    "\n",
    "    # eval_env.step() will execute the action for all the sub-envs, for those where\n",
    "    # the episodes have terminated, the envs will be reset. Hence we use\n",
    "    # \"unfinished\" to track whether the first episode we roll out for each sub-env is\n",
    "    # finished. In contrast, \"done\" only relates to the current episode\n",
    "    # TODO: nao sei pq, mas o unfinished precisa por [0]\n",
    "    episode_return[unfinished] += reward[unfinished[0]].reshape(-1, 1)\n",
    "    #episode_return[unfinished] += reward[unfinished[0]].reshape(-1, 1)\n",
    "\n",
    "    actions[:, -1] = action\n",
    "    state = (\n",
    "        torch.from_numpy(state).to(device=device).reshape(num_envs, -1, state_dim)\n",
    "    )\n",
    "    states = torch.cat([states, state], dim=1)\n",
    "    #print(\"states: {}\".format(states))\n",
    "    # TODO: n sei pq, mas tive que por np.array em reward (na vdd sei, apenas 1 evaluate..)\n",
    "    reward = torch.from_numpy(np.array(reward)).to(device=device).reshape(num_envs, 1)\n",
    "    #reward = torch.from_numpy(reward).to(device=device).reshape(num_envs, 1)\n",
    "    rewards[:, -1] = reward\n",
    "\n",
    "    if mode != \"delayed\":\n",
    "        pred_return = target_return[:, -1] - (reward * reward_scale)\n",
    "    else:\n",
    "        pred_return = target_return[:, -1]\n",
    "    target_return = torch.cat(\n",
    "        [target_return, pred_return.reshape(num_envs, -1, 1)], dim=1\n",
    "    )\n",
    "\n",
    "    timesteps = torch.cat(\n",
    "        [\n",
    "            timesteps,\n",
    "            torch.ones((num_envs, 1), device=device, dtype=torch.long).reshape(\n",
    "                num_envs, 1\n",
    "            )\n",
    "            * (t + 1),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    if t == max_ep_len - 1:\n",
    "        done = np.ones(done.shape).astype(bool)\n",
    "\n",
    "    if np.any(done):\n",
    "        ind = np.where(done)[0]\n",
    "        unfinished[ind] = False\n",
    "        episode_length[ind] = np.minimum(episode_length[ind], t + 1)\n",
    "\n",
    "    if not np.any(unfinished):\n",
    "        break\n",
    "        \n",
    "    calculate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2df21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.get_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfbd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcc9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26570ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Episode Return: {}\".format(episode_return[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b920ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984ae25",
   "metadata": {},
   "source": [
    "## Fim dos Rascunhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5bd6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73875ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0cd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.close()\n",
    "\n",
    "terminal = False\n",
    "\n",
    "while not terminal:\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action = model(env.state) # mode.get_predictions blah\n",
    "    \n",
    "    #action = np.array([3.0,3.0,3.0])\n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec5374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
